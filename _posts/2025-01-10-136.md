---
layout: post
title: '#136 Prosto i praktycznie wyjaÅ›niamy: AI'
date: 2025-01-10 08:00:00 +0200
description: 
episode: "136"
tags: []
spreaker: 63621812
apple: https://podcasts.apple.com/pl/podcast/patoarchitekci/id1477067604?i=1000683426413
newsletter: |
  CzeÅ›Ä‡! ğŸ‘‹ âœ¨

  **"Prosto i praktycznie wyjaÅ›niamy: AI"** to nie _slogan_, a _misja_! Patoarchitekci dekonstruujÄ… Å›wiat **LLM**, **GenAI** i **tokenizacji**. Przygotujcie siÄ™ na _deszcz akronimÃ³w_ i _burzÄ™ mÃ³zgÃ³w_!

  Od **promptÃ³w** po **embeddingi**, od **RAG** do **AI Agents**. Poznajcie _30 kluczowych pojÄ™Ä‡_ AI, ktÃ³re zmieniÄ… Wasze _integracje z LLM_. **Chunking**, **reranking** i **function calling** juÅ¼ nie bÄ™dÄ… _czarnÄ… magiÄ…_!
  
  Chcesz byÄ‡ _AI-native_? PosÅ‚uchaj i _zaimplementuj_! Niech TwÃ³j kod _szepce_ do modeli, a **chatboty** _Å›niÄ…_ o _elektrycznych owcach_. **Patoarchitekci** czekajÄ… â€“ _odpal ten podcast_ szybciej niÅ¼ **LLM** generuje _bzdury_!
  

  
  
---
**"Prosto i praktycznie wyjaÅ›niamy: AI"** to nie _slogan_, a _misja_! Patoarchitekci dekonstruujÄ… Å›wiat **LLM**, **GenAI** i **tokenizacji**. Przygotujcie siÄ™ na _deszcz akronimÃ³w_ i _burzÄ™ mÃ³zgÃ³w_!

Od **promptÃ³w** po **embeddingi**, od **RAG** do **AI Agents**. Poznajcie _30 kluczowych pojÄ™Ä‡_ AI, ktÃ³re zmieniÄ… Wasze _integracje z LLM_. **Chunking**, **reranking** i **function calling** juÅ¼ nie bÄ™dÄ… _czarnÄ… magiÄ…_!

Chcesz byÄ‡ _AI-native_? PosÅ‚uchaj i _zaimplementuj_! Niech TwÃ³j kod _szepce_ do modeli, a **chatboty** _Å›niÄ…_ o _elektrycznych owcach_. **Patoarchitekci** czekajÄ… â€“ _odpal ten podcast_ szybciej niÅ¼ **LLM** generuje _bzdury_!


SÅ‚uchasz PatoarchitektÃ³w dziÄ™ki PROTOPII â€“ firmie, w ktÃ³rej Åukasz i Szymon dziaÅ‚ajÄ… na co dzieÅ„, wspierajÄ…c zespoÅ‚y IT na kaÅ¼dym etapie: od projektowania, przez wdroÅ¼enia i migracje, aÅ¼ po optymalizacjÄ™ i zabezpieczenia. Oferujemy teÅ¼ mentoring i szkolenia dostosowane do potrzeb kaÅ¼dej firmy, niezaleÅ¼nie od wielkoÅ›ci. SprawdÅº nas: [protopia.tech](https://protopia.tech/)

Discord ğŸ‘‰ [https://discord.gg/78zPcEaP22](https://discord.gg/78zPcEaP22)

Linki i ciekawe znaleziska:

- [Protopia - Doradztwo](https://protopia.tech/doradztwo)
- [OpenAI - Tokenizer](https://platform.openai.com/tokenizer)
- [OpenAI - Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [Anthropic - Increase output consistency (JSON mode)](https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/increase-consistency)
- [OpenAI - Function calling](https://platform.openai.com/docs/guides/function-calling)
- [Anthropic - Tool use (function calling)](https://docs.anthropic.com/en/docs/build-with-claude/tool-use)
- [LlamaIndex - Build Knowledge Assistants over your Enterprise Data](https://www.llamaindex.ai/)
- [LangChain](https://www.langchain.com/)
- [Building advanced Retrieval-Augmented Generation systems  ](https://learn.microsoft.com/en-us/azure/developer/ai/advanced-retrieval-augmented-generation)
- [Important concepts and considerations for developers building generative AI solutions  ](https://learn.microsoft.com/en-us/azure/developer/ai/gen-ai-concepts-considerations-developers)
- [Chunk and vectorize by document layout - Azure AI Search  ](https://learn.microsoft.com/en-us/azure/search/search-how-to-semantic-chunking)
- [Building effective agents \ Anthropic](https://www.anthropic.com/research/building-effective-agents)

### Transkrypcja

**Szymon Warda**: CzeÅ›Ä‡! SÅ‚uchacie PatoarchitektÃ³w. ProwadzÄ… Szymon Warda...

**Åukasz KaÅ‚uÅ¼ny**: I Åukasz KaÅ‚uÅ¼ny. Wszystkie linki do tego odcinka znajdziecie na Patoarchitekci.io lub gdzieÅ› tu na dole, jak zawsze ogarniecie. To co? Witamy po krÃ³tkiej przerwie i w nowym roku.

**Szymon Warda**: Tak, cÃ³Å¼ za wspaniaÅ‚ej przerwie.

**Åukasz KaÅ‚uÅ¼ny**: Przynajmniej nie nagrywamy tego o Ã³smej.

**Szymon Warda**: Normalnie wracamy o siÃ³dmej. Dobrze Åukaszu, o czym dzisiaj?

**Åukasz KaÅ‚uÅ¼ny**: Dzisiaj sÅ‚ownik pojÄ™Ä‡, o ktÃ³rym wam wspominaliÅ›my, Å¼e przygotowujemy, a dokÅ‚adniej sÅ‚ownik pojÄ™Ä‡ uÅ¼ycia gotowych LLM-Ã³w, czyli Large Language Models. I patrzyliÅ›my na to i zebraliÅ›my pojÄ™cia, ktÃ³re wystÄ™pujÄ… przy tym, kiedy wykorzystujemy te LLM-y, Å¼eby zbudowaÄ‡ aplikacjÄ™, chatbota, jakieÅ› integracje, inne tego typu elementy. Czyli to co teÅ¼ od Was siÄ™ pojawia w komentarzach: jestem DevOpsem i nie do koÅ„ca rozumiem czym sÄ… te embeddingi, RAG-i, o ktÃ³rych ciÄ…gle wspominacie. WiÄ™c to z naszego takiego codziennego doÅ›wiadczenia wybraliÅ›my 30-32 pojÄ™cia, z ktÃ³rymi siÄ™ zetkniecie i z ktÃ³rymi my na co dzieÅ„ w Protopii mamy do czynienia, kiedy tÅ‚umaczymy caÅ‚Ä… tÄ… budowÄ™. To bÄ™dÄ… najczÄ™stsze pojÄ™cia, ktÃ³re traficie i postaramy Wam siÄ™ pokazaÄ‡ przez flow budowania aplikacji czym sÄ… te podstawowe pojÄ™cia.

**Szymon Warda**: Czyli jednym sÅ‚owem nie silimy siÄ™ na to, Å¼eby zbudowaÄ‡ jeden ostateczny, finalny, koÅ„cowy i najlepszy sÅ‚ownik o LLM-ach, tylko podchodzimy do tego praktycznie.

**Åukasz KaÅ‚uÅ¼ny**: Tak. No i jak lecimy to chyba podstawowe pojÄ™cia, czyli to co wrzucamy do Å›rodka aplikacji, czyli pierwszym pojÄ™ciem, z ktÃ³rym siÄ™ zetkniecie, to bÄ™dzie prompt. I najbardziej prosto okreÅ›lamy takÄ… instrukcjÄ™, polecenie, ktÃ³re przekazujemy do tego modelu AI-owego. Czyli ono okreÅ›la kontekst, co ten model ma wykonaÄ‡. I jeÅ¼eli popatrzycie to moÅ¼e byÄ‡ pytanie, jakaÅ› proÅ›ba, szczegÃ³Å‚owa instrukcja, w zaleÅ¼noÅ›ci co, z czym pracujemy. No i chyba teraz oddam Tobie Szymon gÅ‚os, czym jest tak naprawdÄ™ ten prompt pod spodem i co siÄ™ z nim dzieje.

**Szymon Warda**: No dobrze, no to teraz trochÄ™ wyjdziemy od problemu generalnie, no bo komputery jako komputery to wolÄ… operowaÄ‡ na liczbach, a nie na tekÅ›cie, tekst jest liczbÄ…, wiÄ™c musimy zamieniÄ‡. No i to mamy teraz problemy takie, jak zamieniÄ‡ ten tekst, ktÃ³ry pisaliÅ›my, na liczby? Mamy kilka podejÅ›Ä‡, moÅ¼emy na przykÅ‚ad zrobiÄ‡ taki myk, Å¼e na przykÅ‚ad podzieliÄ‡ per kaÅ¼dy znak unicode'owy, wtedy plusy sÄ… tego takie, Å¼e mamy skoÅ„czony relatywnie doÅ›Ä‡ maÅ‚y zbiÃ³r tych moÅ¼liwoÅ›ci, kombinacji. Czyli zaÅ‚Ã³Å¼my A to jest jedynka i tak dalej, sobie przypisujemy, wiÄ™c tego jest relatywnie maÅ‚o, zakres unicode'u, wiÄ™c nieduÅ¼o. Problem jaki w tym momencie siÄ™ tworzy jest taki, Å¼e nie mamy kontekstu tak naprawdÄ™, nie mamy kontekstu sÅ‚owa, mamy kontekst pojedynczego sÅ‚owa, ale nie zdania i tak dalej. WiÄ™c jest to maÅ‚o wygodne. DrugÄ… skrajnoÅ›ciÄ…, w ktÃ³rÄ… moÅ¼emy wejÅ›Ä‡, to zaÅ‚Ã³Å¼my kontekst caÅ‚ego sÅ‚owa. Czyli dzielimy per biaÅ‚e znaki. Super fajnie, mamy kontekst caÅ‚ego sÅ‚owa, Å‚atwiej siÄ™ operuje i tak dalej, ale sÅ‚Ã³w mamy drastycznie duÅ¼o powiedzmy sobie tak delikatnie. I nagle moÅ¼liwoÅ›Ä‡ tej kombinacji, zakres tych liczb, ktÃ³re musimy przypisaÄ‡ jest bardzo, bardzo duÅ¼y i co wiÄ™cej, bardzo Å‚atwo bÄ™dzie nam coÅ› ominÄ…Ä‡, jeÅ¼eli jakiegoÅ› sÅ‚owa nie widzieliÅ›my. Jest to zero-jedynkowe, czyli jak sÅ‚owo nie wystÄ™powaÅ‚o, to nasz model nie bÄ™dzie mÃ³gÅ‚ o tym myÅ›leÄ‡. No wiÄ™c teraz znajdÅºmy jakiÅ› Å›rodek. MoÅ¼emy tokenizowaÄ‡ to, czyli dzieliÄ‡ na takie tokeny, elementy, ktÃ³rym nasz model bÄ™dzie przypisywaÅ‚ jakÄ…Å› wartoÅ›Ä‡ liczbowÄ…, to sÄ… wÅ‚aÅ›nie tokeny. Element, ktÃ³ry dzieli na te tokeny, to wÅ‚aÅ›nie jest tokenizator, na poziomie takim poÅ›rednim, czyli podsÅ‚owie tak naprawdÄ™. Czyli w tym momencie dzielimy na pewne fragmenty kaÅ¼de sÅ‚owo, czasami one zahaczajÄ… o znaki puste, czasami to sÄ… caÅ‚e sÅ‚owa, czasami zahaczajÄ… o sÅ‚owo, znak pusty i przejÅ›cie do kolejnego sÅ‚owa. Modele sÄ… bardzo rÃ³Å¼ne i tokenatory sÄ… bardzo, bardzo rÃ³Å¼ne. Ale jakie mamy w tym momencie plusy? Po pierwsze, obsÅ‚ugujemy sÅ‚owa, ktÃ³rych nie znamy, bo one mogÄ… skÅ‚adaÄ‡ siÄ™ z kilku tokenÃ³w. Po drugie, rozumiemy kontekst caÅ‚ego sÅ‚owa, ale teÅ¼ czas czÄ™sty kolejnego sÅ‚owa. Rozumiemy znaki puste i specjalne, ktÃ³re dajÄ… siÄ™ mimo wszystko i zbiÃ³r tych danych moÅ¼liwych jest relatywnie niewielki i kontrolujemy go. WiÄ™c tym sÄ… wÅ‚aÅ›nie tokeny i tokenizatory.

**Åukasz KaÅ‚uÅ¼ny**: Ja bym chyba dodaÅ‚ jeszcze taki wizualizacyjny, moÅ¼e wrzucimy tu obraz, ci, ktÃ³rzy oglÄ…dajÄ… wideo, sÅ‚owo friendship, bo Å›wietnie to pokazuje te rozbicie sÅ‚owa friendship na dwa tokeny friend i ship. Tutaj pokazujÄ…ce co to jest.

**Szymon Warda**: Tak, bo widzÄ™, Å¼e w ogÃ³le uÅ¼yÅ‚eÅ› wkÅ‚adu z tej strony, ktÃ³ra wÅ‚aÅ›nie pokazuje jak dziaÅ‚ajÄ… tokenizatory. [niesÅ‚yszalne 00:05:00] 

**Åukasz KaÅ‚uÅ¼ny**: To jest z platform.openai.com/tokenizer. Wrzucimy wam linka i teÅ¼ na ekranie moÅ¼ecie zobaczyÄ‡. FajnÄ… rzeczÄ…, ktÃ³ra jest, dodali to, Å¼e pokazuje ID tokenu. Czyli moÅ¼na sobie pod sÅ‚owem zobaczyÄ‡ teraz jaki token odpowiada danemu sÅ‚owu, jaki to jest zbiÃ³r tokenÃ³w.

**Szymon Warda**: Dla mnie duÅ¼o fajniejsze w tej stronie jest to, Å¼e on pokazuje jak siÄ™ rÃ³Å¼ne tokenizatory dla rÃ³Å¼nych modeli, bo tam w prawym gÃ³rnym rogu jest dropdpwn, ktÃ³ry pokazuje jak ktÃ³ry model tokenizuje, jak to w ogÃ³le wyglÄ…da. I na podstawie rÃ³Å¼nic fajnie faktycznie widaÄ‡ podejÅ›cie do tego. OgÃ³lnie bÄ™dzie w opisie, zerknijcie jak bÄ™dzie was interesowaÅ‚o.

**Åukasz KaÅ‚uÅ¼ny**: Dobra.

**Szymon Warda**: Sporo daje.

**Åukasz KaÅ‚uÅ¼ny**: I teraz lecimy, bo pojawia siÄ™ coÅ› takiego jak trzy pojÄ™cia, one sÄ… ze sobÄ… zebrane, jeÅ¼eli mÃ³wimy. SÄ… input tokeny, output tokeny i context window. Czyli input token to jest liczba tokenÃ³w, ktÃ³re my moÅ¼emy wysÅ‚aÄ‡ w tym prompcie do Å›rodka tego modelu. Czyli jeÅ¼eli coÅ› tam wpisujecie, doÅ‚Ä…czacie do tego dane, to jest jakaÅ› iloÅ›Ä‡. Czyli mÃ³wimy, Å¼e np. model moÅ¼e przyjÄ…Ä‡ osiem tysiÄ™cy, co jest teraz takim standardem, osiem tysiÄ™cy tokenÃ³w wejÅ›ciowych i potem moÅ¼emy powiedzieÄ‡...

**Szymon Warda**: Co jest teÅ¼ czÄ™sto elementem wyceny. Modele, ktÃ³re majÄ… dÅ‚uÅ¼szÄ… liczbÄ™ tokenÃ³w przyjmujÄ… czÄ™sto sÄ… po prostu droÅ¼sze.

**Åukasz KaÅ‚uÅ¼ny**: Tak i potem macie output tokeny, czyli ile tokenÃ³w ten model moÅ¼e z siebie maksymalnie wyrzuciÄ‡. I teraz waÅ¼nÄ… rzeczÄ…, pojawia siÄ™ coÅ›, co nazywamy context window, czyli ile moÅ¼emy zrobiÄ‡ input tokenÃ³w i output tokenÃ³w, Å¼eby przetworzyÄ‡, czyli ile model z tego wykorzysta. I rzecz, z ktÃ³rÄ… trzeba siÄ™ pogodziÄ‡, bo czÄ™sto sÅ‚yszymy, Å¼e jest jakiÅ› kontekst, jakaÅ› historia jak uÅ¼ywamy chatGPT czy CloudA'Ä™. I to jest trochÄ™ kÅ‚amstwo dla dzieci, poniewaÅ¼ tak naprawdÄ™ za kaÅ¼dym razem model jest bezstanowy, jego odpowiedzi sÄ… bezstanowe, wiÄ™c my do Å›rodka wysyÅ‚amy historiÄ™ poprzednich pytaÅ„ i odpowiedzi, jako zaÅ‚Ä…czajÄ…c w specjalny sposÃ³b, Å¼e: hej drogi modelu, teraz mam takie pytanie, ale ja z tobÄ… rozmawiaÅ‚em o tym. I teraz suma input i output tokenu nie moÅ¼e przekroczyÄ‡ tego okna kontekstowego w sumie. To jest istotny element.

**Szymon Warda**: NaprowadziÅ‚eÅ› teraz na completion, bo teraz skoro powiedziaÅ‚eÅ› jak to dziaÅ‚a, Å¼e faktycznie model generuje token po tokenie tak naprawdÄ™, dziÄ™ki czemu tak teÅ¼ pokazuje Å‚adnie to chatGPT, jak ta strona, wchodzimy, Å‚adnie generuje sobie po kolei. No to wÅ‚aÅ›nie w tym momencie dla modelu nie ma znaczenia czy on generuje kolejny znak na zasadzie tego, Å¼e to byÅ‚o: ej, uzupeÅ‚nij mi co bÄ™dzie kolejne czy po prostu jako faza tej pÄ™tli i odpowiedzi tak naprawdÄ™. I wchodzimy wÅ‚aÅ›nie w completion, czyli ten element, Å¼e on dogenerowuje token po tokenie i moÅ¼emy korzystaÄ‡ z tego, Å¼e go uzupeÅ‚ni, albo po prostu tak w ogÃ³le generuje odpowiedÅº. Dobrze, message, message Åukaszu.

**Åukasz KaÅ‚uÅ¼ny**: Tak, jak jesteÅ›my, powiedzieliÅ›my sobie, Å¼e on wybiera najbardziej prawdopodobne nastÄ™pne tokeny, to completition...

**Szymon Warda**: O tym jeszcze powiemy.

**Åukasz KaÅ‚uÅ¼ny**: Tak, to completition jest takim podstawowym wÅ‚aÅ›nie efektem, ktÃ³ry uÅ¼ywaliÅ›my. I pojawiÅ‚o siÄ™ teraz w wielu modelach coÅ›, co nazywamy chat completition. I tutaj chodzi o to, Å¼e ten sposÃ³b wysÅ‚ania danych do modelu jest ustrukturyzowany w message. I to, co my sobie wpisujemy w chat'cie, to nazywamy najczÄ™Å›ciej user prompt, user message. I to jest po prostu nasza wiadomoÅ›Ä‡, ktÃ³rÄ… my wysyÅ‚amy do modelu, ktÃ³ra zawiera jakieÅ› polecenie dane do przetworzenia. CzÄ™sto moÅ¼e zawieraÄ‡ poprzednie wiadomoÅ›ci, ktÃ³re stajÄ… siÄ™ wÅ‚aÅ›nie czÄ™Å›ciÄ… kontekstu, Å¼eby dawaÅ‚ to, Å¼e ten model myÅ›li. JeÅ¼eli popatrzycie, to odpowiedÅº z takiego modelu nazywamy assistant message, czyli to co nam model odpowiada. I pod spodem pojawia siÄ™ jeszcze trzecie, ktÃ³re jest najwaÅ¼niejsze, system message, system prompt, czyli polecenia jak ten model tak naprawdÄ™ ma siÄ™ zachowywaÄ‡. Z zaÅ‚oÅ¼enia tam wrzucamy te wszystkie elementy, ktÃ³re mÃ³wiÄ… w jaki sposÃ³b masz reagowaÄ‡. Czyli jesteÅ› przyjaznym asystentem, ktÃ³ra jest najczÄ™stsza. System prompt od chatGPT czy CloudA to jest kilka stron tekstu w jaki sposÃ³b ma siÄ™ zachowywaÄ‡. MoglibyÅ›my powiedzieÄ‡, jeÅ¼eli mamy jakieÅ› integracje w takim modelu, do czego potem przejdziemy pod koniec, to moglibyÅ›my powiedzieÄ‡: masz dostÄ™pne narzÄ™dzia XYZ. Jak uÅ¼ytkownik zapyta siÄ™ o to co jest na narzÄ™dziu, to wyszukaj to np. w Bingu czy w Wikipedii, tych sÅ‚Ã³w kluczowych i to nam steruje caÅ‚ym wykorzystaniem.

**Szymon Warda**: Teraz przejdÅºmy kawaÅ‚ek do tego co juÅ¼ zahaczyliÅ›my, czyli tak naprawdÄ™ jak te modele potuningowaÄ‡. OgÃ³lnie zmianÄ™ parametrÃ³w modelu, to Å¼e jak on ma siÄ™ zachowywaÄ‡, osiÄ…gamy przez hiperparametry, hyperparameters. I to mamy takie trzy najwaÅ¼niejsze: temperatura, top K i top P. O co w ogÃ³le z tym chodzi? No jak juÅ¼ powiedziaÅ‚eÅ› i powiedzieliÅ›my sobie, Å¼e wybieramy sobie jakieÅ› tokeny. No ok, ale wiemy, Å¼e np. po sÅ‚owie kasza bÄ™dziemy mieli caÅ‚y zbiÃ³r: mamy gryczana, manna i pozostaÅ‚e rzeczy. No i teraz problemy jakie mamy, to jest to, jak ten model bÄ™dzie wybieraÅ‚. Po pierwsze, ile bÄ™dzie generowaÅ‚ moÅ¼liwoÅ›ci. Po drugie, jakie sÅ‚owa bÄ™dzie wybieraÅ‚. Po trzecie, ktÃ³re sÅ‚owo wybierze finalnie. Dobra i do tego wÅ‚aÅ›nie mamy, wyobraÅºmy sobie, Å¼e jesteÅ›my w restauracji. Mamy dwie metody samplingu tak naprawdÄ™, takie skrajnoÅ›ci. Gridy, to jest takie, Å¼e wykorzystamy zawsze najbardziej prawdopodobnÄ… odpowiedÅº, czyli ta ktÃ³ra z reguÅ‚y wystÄ™puje. Czyli zaÅ‚Ã³Å¼my, jak jesteÅ›my w jakiejÅ› restauracji bÄ™dziemy zamawiali zawsze to samo najbardziej popularne danie. SkrajnoÅ›ciÄ… drugÄ… jest random sampling, czyli totalna losowoÅ›Ä‡. To powoduje, Å¼e token jeden moÅ¼e nie mieÄ‡ porÃ³wnania z drugim po prostu. Z tych wszystkich tokenÃ³w, ktÃ³re dostaliÅ›my, wybieramy ktÃ³rykolwiek. No i teraz jak to kontrolowaÄ‡? Mamy top K i top P. Zacznijmy od top K. Top K, przede wszystkim dostajemy tokeny. Co robi top K? Top K wycina, czyli bierze te tokeny, bierze K tokenÃ³w, ktÃ³re dostaÅ‚. No okej, potem dalej to leci do top P. Co robi top P? To P jest ogÃ³lnie sprytne, bo top P robi taki myk, Å¼e idzie po tych tokenach po kolei sortujÄ…c po prawdopodobieÅ„stwie i zbierze token po tokenie, tokenie i sumuje ich prawdopodobieÅ„stwo, aby sumaryczne prawdopodobieÅ„stwo, czyli masa probabilistyczna, byÅ‚a powyÅ¼ej wartoÅ›ci P. Czyli zbieramy sÅ‚owa, ktÃ³re sÄ… prawdopodobne do pewnego stopnia. No i dobra, kolejnym elementem to jest temperatura. Co robi temperatura? Temperatura robi taki myk, Å¼e moÅ¼e byÄ‡ poniÅ¼ej 0 lub powyÅ¼ej 0. JeÅ¼eli patrzymy na to co dalej model robi z naszymi wagami probabilistycznymi, to jest to, Å¼e on bierze tÄ… wagÄ™ i dzieli przez temperaturÄ™. Jaki jest tego efekt? Efekt jest tego taki, Å¼e jeÅ¼eli mamy temperaturÄ™ poniÅ¼ej zera, czyli dzielimy przez wartoÅ›ci od zero przecinek jakaÅ› wartoÅ›Ä‡ do jedynki, to w tym momencie rozszerzamy zakresy odlegÅ‚oÅ›ci tych sÅ‚Ã³w. Czyli ten model, mniejsza szansa, Å¼e zahaczy jedno sÅ‚owo o drugie, bo one sÄ… bardziej rozdzielone, sÄ… bardziej deterministyczne, wszystko fajnie. Natomiast jeÅ¼eli dzielimy przez wartoÅ›Ä‡ wiÄ™kszÄ… od jedynki, wÅ‚aÅ›ciwie moÅ¼na tak powiedzieÄ‡, to w tym momencie co siÄ™ dzieje? To w tym momencie zbliÅ¼amy te wartoÅ›ci do siebie. I teraz Å‚Ä…czymy, teraz poÅ‚Ä…czmy to razem, jak to wyglÄ…da. Jak dziaÅ‚a taki model? Pierwsze, to on liczy prawdopodobieÅ„stwo dla kaÅ¼dego tokenu, potem bierze pod uwagÄ™ temperaturÄ™ i zmienia te finalne wagi. Potem aplikuje top K, czyli wybiera top K najbardziej prawdopodobnych tokenÃ³w. Potem aplikuje top P, czyli sumuje aÅ¼ pewna wartoÅ›Ä‡ bÄ™dzie osiÄ…gniÄ™ta. Potem normalizuje to prawdopodobieÅ„stwo, a na koniec wybiera finalny token tak naprawdÄ™ i tak wÅ‚aÅ›nie go otrzymujemy.

**Åukasz KaÅ‚uÅ¼ny**: Tak, to jest caÅ‚oÅ›Ä‡, jak to wyglÄ…da pod spodem, a najczÄ™Å›ciej operujemy na testach na suwaku od temperatury po prostu, jeÅ¼eli popatrzymy.

**Szymon Warda**: Z reguÅ‚y tak.

**Åukasz KaÅ‚uÅ¼ny**: I zwykle efekty bÄ™dÄ… wystarczajÄ…ce. Jak przechodzimy do efektÃ³w, teraz pojÄ™cie, na ktÃ³rÄ… jest klauzula sumienia, prawdopodobnie bardziej opÅ‚aca Wam siÄ™ napisaÄ‡ dobrze zrobienie dobrego prompta i zadbanie o dobrego jakoÅ›ciowego RAG-a, czyli dostawianie tych danych z naszych dokumentÃ³w i innych rzeczy, niÅ¼ fine tuning, czyli rzecz, ktÃ³ra jest mokrym snem: wytrenujemy wÅ‚asny model. JeÅ¼eli mÃ³wimy o gotowych LLM-ach, to fine tuning to jest dostarczanie specyficznego wÅ‚aÅ›nie zestawu danych treningowych dotyczÄ…cego naszego problemu, ktÃ³ry my mamy. I taki format najczÄ™Å›ciej dostarczamy w postaci JSON L, czyli to jest multiline, taki JSON, w ktÃ³rym bÄ™dzie pytanie i oczekiwana odpowiedÅº. I on nadpisuje to, czego model jest nauczony. Tylko Å¼eby miaÅ‚o to sens w zaleÅ¼noÅ›ci od tego, ktÃ³ry gotowy model wybieramy, to trzeba dostarczyÄ‡ dobrych jakoÅ›ciowo danych, takich pytaÅ„ i odpowiedzi, moÅ¼na powiedzieÄ‡, Å¼e od groma.

**Szymon Warda**: DuÅ¼o, bardzo duÅ¼o.

**Åukasz KaÅ‚uÅ¼ny**: Tak. I potem przez to, Å¼e te modele siÄ™ zmieniajÄ…, np. jeÅ¼eli korzystamy z gotowych modeli jako API, to teÅ¼ trzeba pamiÄ™taÄ‡ o caÅ‚ym procesie aktualizowania tego data setu, sprawdzania, itd. StÄ…d z naszego doÅ›wiadczenia na poczÄ…tku lepiej siÄ™ skupiÄ‡ na dobrym zbudowaniu wÅ‚aÅ›nie promptÃ³w i dobrym Å‚adowaniu tych danych i trzymaniu. WiÄ™c chyba przejdÅºmy sobie do tego czym jest tak naprawdÄ™ prompting.

**Szymon Warda**: A dokÅ‚adnie prompt engineering. Ja tu bym powiedziaÅ‚, Å¼e prompt engineering to jest temat wiÄ™kszoÅ›ci szkoleÅ„ online obecnie, Å¼e tak powiem. No ale tak naprawdÄ™ to nad tym siÄ™ duÅ¼o, duÅ¼o, duÅ¼o takich gorszych jakoÅ›ciowo szkoleÅ„ skupia. Ale okej, ale o co chodzi? Mi to bardzo mocno przypomina fakt, jak jeszcze wiele, wiele lat temu, poniewaÅ¼ juÅ¼ nie jesteÅ›my mÅ‚odymi ludÅºmi, mimo tego co byÅ›my chcieli uwaÅ¼aÄ‡, to jest to jak siÄ™ uczyliÅ›my caÅ‚ej skÅ‚adni zapytaÅ„ googlowych, co ktÃ³re znaki oznaczajÄ… i jak to w ogÃ³le zrobiÄ‡. No to czym jest prompt engineering? Prompt engineering jest procesem dopasowywania zapytania w jÄ™zyku naturalnym do modeli LLM-owych, naszych duÅ¼ych modeli, Å¼eby otrzymaÄ‡ okreÅ›lony wynik. Dostosowywanie sÅ‚Ã³w, dostosowywanie formy, dostosowywanie wszystkiego co potrzebujemy, Å¼eby ten model zachowywaÅ‚ siÄ™ prawidÅ‚owo tak naprawdÄ™. I tak co Åukasz powiedziaÅ‚, moÅ¼e to nie brzmi seksownie, bo wiadomo, Å¼e fine tuning i trenowanie modelu brzmi duÅ¼o lepiej i kaÅ¼dy by siÄ™ chciaÅ‚ tym zajÄ…Ä‡, ale realnie prompt engineering to jest ten obszar, ktÃ³ry wam da najwiÄ™cej wartoÅ›ci. MaÅ‚o kto bÄ™dzie potrzebowaÅ‚ wejÅ›Ä‡ w fine tuning tak naprawdÄ™.

**Åukasz KaÅ‚uÅ¼ny**: Tak. I jeÅ¼eli teraz pÃ³jdziemy to tam pojawi siÄ™ kilka technik podejÅ›cia do tego. PierwszÄ… jest Zero-Shot Learning. Czyli po prostu wpisujemy instrukcjÄ™, mÃ³wimy co ma zrobiÄ‡, opisujemy precyzyjnie zadanie, ale nie mÃ³wimy jak ma je wykonaÄ‡, cokolwiek i liczymy, Å¼e ta odpowiedÅº bÄ™dzie prawidÅ‚owa. Problem z tym jest taki, Å¼e nie mamy w miarÄ™ stabilnoÅ›ci tej odpowiedzi. Czyli to jest taka Å›wietna rzecz, Å¼e bierzemy i zaczynamy sobie dyskusjÄ™ na chat'cie, ale raczej nie oczekujemy tego w aplikacji. I tu pojawia siÄ™ taki zÅ‚oty Å›rodek, ktÃ³ry nazywamy Few-Shot Learningiem. Czyli technika, gdzie to wszystko dokÅ‚adnie opisujemy co ma byÄ‡ zrobione, ale oprÃ³cz tego w tym prompcie dostarczamy od dwÃ³ch do piÄ™ciu przykÅ‚adÃ³w jaki ma byÄ‡ wynik tego zadania, jaki ma byÄ‡ efekt. I to jest taki zÅ‚oty Å›rodek pomiÄ™dzy: wrzucÄ™ coÅ› na paÅ‚Ä™ i bÄ™dzie dziaÅ‚aÅ‚o, a pomiÄ™dzy wrzucÄ™ tonÄ™ tokenÃ³w na inpucie, Å¼eby dokÅ‚adnie doprecyzowaÄ‡. I ten Few-Shot Learning bÄ™dzie najczÄ™Å›ciej wykorzystywanÄ… technikÄ…. I sÅ‚uchajcie i teraz taka rzecz, ktÃ³rÄ… wykorzystujÄ™ w codziennej pracy, bo sÅ‚uchajcie prompt engineering to nie jest Å¼adna, Å¼adna magia. I to, co zwykle polecam robiÄ‡, moÅ¼ecie teÅ¼ to zobaczyÄ‡ na naszym Discordzie, ktÃ³rÄ… pokazuje tam Pato od kuchni, jak nasz podcast jest zautomatyzowany pod spodem, jeÅ¼eli chodzi o wydawanie.

**Szymon Warda**: Ja to nazywam kanaÅ‚em: gdzie Åukasz tÅ‚umaczy, czemu coÅ› nie zadziaÅ‚aÅ‚o jak powinno zadziaÅ‚aÄ‡.

**Åukasz KaÅ‚uÅ¼ny**: Tak albo wylewa swoje Å¼ale, Å¼e coÅ› siÄ™ zmieniÅ‚o w modelach Anthropicu. Ale wracajÄ…c do tego, to sÅ‚uchajcie, taki prompt, jeÅ¼eli macie przykÅ‚adowy szablon prompta i chcecie go dopieÅ›ciÄ‡, to ja go wrzucam wÅ‚aÅ›nie w CloudA'Ä™ czy w chatGPT i proszÄ™ go o rozbudowanie i doprecyzowanie. Czyli mam swÃ³j wstÄ™pny prompt i potem tuningujÄ™ go sobie LLM-em, Å¼eby byÅ‚ lepszy, w szczegÃ³lnoÅ›ci dopracowujÄ™... NajgorszÄ… rzeczÄ… sÄ… te przykÅ‚ady. WiÄ™c proÅ›ciej jest nam powiedzieÄ‡: wygeneruj mi kilka przykÅ‚adÃ³w i powiedzieÄ‡ co ma w nich zmieniÄ‡, Å¼eby byÅ‚o dobrze i dopiero je potem wykorzystaÄ‡. To jest taki zÅ‚oty Å›rodek w tej technice budowy, wiÄ™c nie musicie lecieÄ‡ na wszystkie kursy prompt engineeringu, bo sprowadzÄ… siÄ™ do, jak to siÄ™ Å‚adnie mÃ³wi, meta prompt engineeringu.

**Szymon Warda**: Dobrze, to teraz lecimy, bo mÃ³wiÅ‚eÅ›, Å¼e wÅ‚aÅ›nie Few-Shot Learning jest zÅ‚otym Å›rodkiem. Ja bym powiedziaÅ‚, Å¼e nie do koÅ„ca. Bo czemu, o co chodzi? Mamy caÅ‚y Chain-of-Thought, CoT. O co w tym chodzi? Chodzi o to, Å¼e pewne zdania lepiej jest rozbijaÄ‡ na mniejsze problemy. Wiem, wielka oczywistoÅ›Ä‡ zostaÅ‚a wypowiedziana. Co jest w tym momencie waÅ¼ne? To jest to, Å¼e to siÄ™ Å‚adnie skupia, zaÅ‚Ã³Å¼my zadajÄ…c modelowi pytania odnoÅ›nie obliczenia czegoÅ›, wywnioskowania czegoÅ›, gdzie ta liczba krokÃ³w, nawet o ktÃ³rych byÅ›my my myÅ›leli, bÄ™dzie trochÄ™ wiÄ™ksza. I Chain-of-Thought wÅ‚aÅ›nie to jest takie powiedzenie modelowi: okej, to wpierw rozbij to na poszczegÃ³lne kroki, a pÃ³Åºniej odpowiedz na kaÅ¼dy krok po kolei biorÄ…c pod uwagÄ™ wynik poprzedniego kroku. I teraz jak to rozbicie na kolejne kroki moÅ¼e siÄ™ odbywaÄ‡? Ano moÅ¼e odbywaÄ‡ siÄ™ na dwa sposoby. Albo to model sam wpierw rozbija, albo wÅ‚aÅ›nie korzystamy z Zero-Shot Learning, gdzie dajemy modelowi, zaÅ‚Ã³Å¼my Å¼e: dla takiego problemu rozbijasz na takie kroki i on dziÄ™ki temu doucza siÄ™, wie juÅ¼ co zrobiÄ‡ tak naprawdÄ™. Bardzo przydatne Chain-of-Thought jest w kontekÅ›cie tego zrozumienia jak model do tego doszedÅ‚. To jest przydatne, nawet dla tego ludzkiego elementu. I ma zastosowanie wÅ‚aÅ›nie gÅ‚Ã³wnie przy takich rzeczach logicznych, przydaje siÄ™ bardzo.

**Åukasz KaÅ‚uÅ¼ny**: Tak i tutaj idÄ…c za tym pojawia siÄ™ coÅ› co nazywamy prompt chainningiem, czyli budujemy sobie Å‚aÅ„cuch promptÃ³w. To jest wÅ‚aÅ›nie taka juÅ¼ implementacja faktycznie. IdÄ…c na przykÅ‚ad, jak ja to wykorzystujÄ™ przy podcaÅ›cie. RozdziaÅ‚y, ktÃ³re widzicie, np. jak teraz oglÄ…dacie ten odcinek na YouTubie, nie sÄ… generowane przeze mnie w Å¼aden sposÃ³b, czy Szymona, czy przez czÅ‚owieka, tylko dokonuje tego CloudA 3.5 Sonet, czyli model od Anthropica jako API. I on np. gdybyÅ›my dali mu to za jednym razem, dali mu po prostu transkrypcjÄ™, zrobiÅ‚by to beznadziejnie. O tak, co zresztÄ… prÃ³bowaÅ‚em zrobiÄ‡ w jednym prompcie. WiÄ™c tutaj co siÄ™ dzieje, to tak jak tworzymy na przykÅ‚adzie naszego podcastu, wrzucamy transkrypcjÄ™ ze znacznikami czasu w formie SRT, do ktÃ³rej jest pierwsza sekwencja, to jest pogrupuj wszystkie wÄ…tki, ktÃ³re byÅ‚y w podcaÅ›cie w takie maÅ‚e, daj znaczniki kiedy siÄ™ zaczÄ…Å‚, kiedy skoÅ„czyÅ‚, dodaj cytaty. NastÄ™pnÄ… czÄ™Å›ciÄ…, ktÃ³ra siÄ™ dzieje, to pogrupuj to w duÅ¼e rozdziaÅ‚y. I ostatnim promptem, ktÃ³ry wychodzi potem, korzysta z tego, jest wygeneruj mi napisy juÅ¼ w postaci HH:MM:SS plus nazwa tego rozdziaÅ‚u. Czyli zobaczcie caÅ‚e te zadanie, tak jak Szymon powiedziaÅ‚ o Chain-of-Thought i rozbijaniu, na koniec technicznie robi siÄ™ to, Å¼e wygeneruj mi takie np. rozdziaÅ‚y do rozdziaÅ‚y. To sÄ… trzy prompty, ktÃ³ry kaÅ¼dy nastÄ™pny korzysta z poprzedniego polecenia. WiÄ™c ja wysyÅ‚am trzy strzaÅ‚y do takiego API LLM-a. Pierwsze jest wyciÄ…gnij mi wszystkie wÄ…tki w podcaÅ›cie, nastÄ™pnie pogrupuj je logicznie, a trzeci dopiero wygeneruj znaczniki czasowe na podstawie tego, co ten ostatni ci zwrÃ³ciÅ‚.

**Szymon Warda**: Dobra, to lecimy do RAG-Ã³w, czyli elementu z ktÃ³rego bÄ™dziecie korzystali, na pewno bÄ™dziecie mieli najwiÄ™cej wartoÅ›ci.

**Åukasz KaÅ‚uÅ¼ny**: Tak, moÅ¼e jeszcze czym jest pojÄ™cie RAG? Czym jest pojÄ™cie RAG, bo zawsze uÅ¼ywamy skrÃ³conego.

**Szymon Warda**: To Åukasz daj, wytÅ‚umacz, bo ty bardzo, bardzo lubisz.

**Åukasz KaÅ‚uÅ¼ny**: Retrieval Augmented Generation. Ale pÃ³jdÅºmy teraz w detale, bo taka jest wÅ‚aÅ›ciwa nazwa, a chodzi o detale tej techniki.

**Szymon Warda**: Dobra, to zacznijmy od podstaw, podstaw, Å¼ebyÅ›cie rozumieli. MÃ³wiliÅ›my sobie, Å¼e maszyny lubiÄ… operowaÄ‡ sobie na liczbach. No okej, mieliÅ›my sobie te tokeny. Tokenom moÅ¼emy przypisaÄ‡ jakÄ…Å› liczbÄ™. Ale to nie jest takie proste, poniewaÅ¼ o ile w najprostszej wersji moglibyÅ›my zrobiÄ‡ coÅ› takiego wÅ‚aÅ›nie, Å¼e tokenowi przypisujemy liczbÄ™, to realnie w tym momencie wracamy do czego? Wracamy do Lucyny czyli Solara czyli Elastica. Upraszczam bardzo mocno, proszÄ™ nie rzucajcie kamieniami w komentarzach, bo to jest uproszczenie. Wiem, Å¼e Lucyna dziaÅ‚a odrobinkÄ™ inaczej, ale bear with me, jak to mÃ³wiÄ… Anglicy. Czyli pewnemu tokenowi przypisujemy jakÄ…Å› liczbÄ™. Okej, byÅ›my zrobili to tylko w ten sposÃ³b, no to fajnie, ale tracimy ten kontekst, ktÃ³ry siÄ™ dzieje dookoÅ‚a. A wÅ‚aÅ›nie na tym kontekÅ›cie nam zaleÅ¼y. WiÄ™c chodzi wÅ‚aÅ›nie o to, Å¼eby zamieniÄ‡ sensownie i z pewnÄ… wartoÅ›ciÄ… nasz token na liczbÄ™. Jak to moÅ¼emy w ogÃ³le zrobiÄ‡? MoÅ¼e cofnijmy siÄ™ moÅ¼e wczeÅ›niej, czemu wÅ‚aÅ›ciwie to robimy? Ano dlatego, Å¼eby mÃ³c powyszukiwaÄ‡ sÅ‚owa podobne i sÅ‚owa, ktÃ³re wystÄ™pujÄ… koÅ‚o siebie, sÅ‚owa, ktÃ³re wystÄ™pujÄ… jeden po drugim, stworzyÄ‡ takie kohorty gdzie one, ktÃ³re wystÄ™pujÄ…. Dobra, to teraz Å¼eby wam daÄ‡ kontekst jak siÄ™ rozwijaÅ‚ ten sam proces. On siÄ™ rozwijaÅ‚ bardzo mocno, chociaÅ¼ ze wzglÄ™du np. na LucynÄ™. Mamy tÄ… zamianÄ™. Np. najprostsza, najstarsza wersja, mamy prosty word embedding, czyli robimy Word2vec albo Gloss. Ogarniamy kontekst konkretnego zdania, moÅ¼e dokumentu. Fastex ogarnia odmianÄ™, czyli tzw. morfologizacjÄ™, czyli ucinamy polskie koÅ„cÃ³wki fleksyjne. Teraz mamy Context Word Embeddings. Tam mamy te modele, ktÃ³re zaczniecie juÅ¼ poznawaÄ‡ po nazwach, ktÃ³re czasami wystÄ™pujÄ…. Mamy ELMo, czyli on w tym momencie dodaje kontekst do konkretnego tokenu, daje kontekst caÅ‚ego swojego zbioru w ktÃ³rym wystÄ™puje. PrzykÅ‚ad czemu to jest waÅ¼ne i np. moment, gdzie np. Elastic albo Solar wymiÄ™kajÄ…. Polskie sÅ‚owo "soli". Tak jak mÃ³wimy np.: nie ma soli. Nie wiemy czy chodzi o sÃ³l, jako przyprawÄ™, czy chodzi o rybÄ™. Dopiero z tym kontekstem globalnym jesteÅ›my w stanie dowiedzieÄ‡ siÄ™ o czym wÅ‚aÅ›ciwie byÅ‚a mowa. Dobra, potem mamy jeszcze BERT-a, ktÃ³ry ogarnia co jest przed i co jest po danym sÅ‚owie, czyli dodaje do kontekstu. Mamy teÅ¼ GPT. Brzmi znajomo, prawda? No to on uÅ¼ywa tylko to co jest po lewej stronie sÅ‚owa, prawda? Bo on generuje kolejne, kolejne, kolejne, tokeny. Tych sposobÃ³w jest doÅ›Ä‡ sporo. Jest Sentence Embedding, jest Document Embedding, jest Transformer-based Embedding, jeszcze sÄ… Combined Approaches, czyli Å‚Ä…czymy rÃ³Å¼ne podejÅ›cia i mieszamy tymi embeddingami. Ale caÅ‚y clue polega wÅ‚aÅ›nie na tym, jak zÅ‚apaÄ‡ kontekst tak, Å¼eby dalej sÅ‚owo byÅ‚o czytelne, o czym opisujemy, ale teÅ¼ mieÄ‡ jak najwiÄ™kszy kontekst tego, co byÅ›my chcieli opisywaÄ‡ tak naprawdÄ™.

**Åukasz KaÅ‚uÅ¼ny**: Tak i w zaleÅ¼noÅ›ci potem od tego API, ktÃ³re wybierzecie do budowania tych osadzeÅ„ w bazie, budowy tych wektorÃ³w, one majÄ… juÅ¼ swÃ³j model wypracowany tego, jeÅ¼eli na to popatrzymy.

**Szymon Warda**: No wiÄ™c teraz podejdÅºmy do tych wektorÃ³w. To jest po prostu wektor, ktÃ³ry ma jeden rzÄ…d i bardzo, bardzo, bardzo wiele kolumn. I teraz gdzie te parametry przechowujemy? W bazach wektorowych. Dlatego przez ten caÅ‚y rok mieliÅ›my tyle informacji, Å¼e kaÅ¼da baza byÅ‚a bazÄ… wektorowÄ…, od Postgresa przez Redisa przez kaÅ¼dÄ… innÄ… bazÄ™. A no teraz czemu wÅ‚aÅ›ciwie mamy w takim razie te wektory? Z bardzo prostego powodu, poniewaÅ¼ na wektorach bardzo Å‚atwo liczy siÄ™ podobieÅ„stwo. Czyli bierzemy takie dwa wielowymiarowe wektory sobie opisujÄ…ce jak to siÄ™ zachowuje i liczymy miÄ™dzy nimi podobieÅ„stwo. NajprostszÄ… formÄ… liczenia prawdopodobieÅ„stwa jest liczenie cosinusa pomiÄ™dzy tymi dwoma wektorami. Wiem, Å¼e teraz sÅ‚yszy siÄ™ co innego, itd., ale upraszczam, po prostu liczymy kont rozwarcia miÄ™dzy wektorami.

**Åukasz KaÅ‚uÅ¼ny**: Tak, najproÅ›ciej teÅ¼ zwizualizowaÄ‡ wektor czasami na mapie i pokazaÄ‡ go. To jest w bardzo uproszczony sposÃ³b, Å¼e sÅ‚owa sÄ… rozrzucone po takim wykresie 3D i sÄ… to punkty w przestrzeni. Ja to bardzo Szymon upraszczam, ale Å¼eby zwizualizowaÄ‡ to na poczÄ…tek.

**Szymon Warda**: Ja bardziej upraszczam to na poziomie takiego dwuwymiarowego, czyli mamy trÃ³jkÄ…t. Jak wektory sÄ… do siebie podobne, jeÅ¼eli to sÄ… te same wektory, to generalnie kÄ…t miÄ™dzy nimi bÄ™dzie 0 tak naprawdÄ™. JeÅ¼eli bÄ™dÄ… caÅ‚kowicie inne, no to mamy kÄ…t dziewiÄ™Ä‡dziesiÄ…t stopni, prawda? I operujemy w tym zakresie. Dlatego wÅ‚aÅ›nie tak bardzo mÃ³wi siÄ™ tak duÅ¼o o bazach wektorowych, Å¼eby wÅ‚aÅ›nie liczyÅ‚o siÄ™ prawdopodobieÅ„stwo. Nie tylko prawdopodobieÅ„stwo liczymy i do czego moÅ¼na to zastosowaÄ‡? Bo to trochÄ™ wiÄ™cej moÅ¼na zrobiÄ‡. MoÅ¼emy znalezienie obiektu podobnego, do tego, ktÃ³ry podaliÅ›my, czyli dajemy jakiÅ› dokument, mÃ³wimy: ej znajdÅº mi podobne. Brzmi znajomo. MoÅ¼emy teÅ¼ pÃ³jÅ›Ä‡ na poziomie dokumentu, moÅ¼emy powiedzieÄ‡: to sÅ‚owo, znajdÅº mi podobne dokumenty gdzie wystÄ™pujÄ… podobne sÅ‚owa, podobne wektory. To samo korzystajÄ… oczywiÅ›cie modele LLM-owe, one zapodajÄ… i mÃ³wiÄ… daj mi kolejne. Szukanie anomalii, czyli dajemy dokumenty i mÃ³wimy: znajdÅº mi jak najbardziej inny albo te, ktÃ³re odstajÄ… od siebie. Wyszukiwanie rÃ³Å¼nych rzeczy, ktÃ³rych nie powinno byÄ‡ w danym zbiorze. MoÅ¼e go trochÄ™ byÄ‡. Kolejny to jest duplikacja, czyli np. caÅ‚e wyszukiwanie plagiatÃ³w. No to wÅ‚aÅ›nie na tym polega. JeÅ¼eli dwa dokumenty sÄ… dzisiaj bardzo, bardzo podobne to znaczy, Å¼e coÅ› tam musiaÅ‚o zajÅ›Ä‡ tak naprawdÄ™. Albo ewentualnie usuwanie, to co my sami robimy, usuwanie dokumentÃ³w podobnych, usuwanie danych, ktÃ³re duplikujÄ… siÄ™ w pewien sposÃ³b.

**Åukasz KaÅ‚uÅ¼ny**: MÃ³wimy teraz caÅ‚y czas o dokumentach, jak zaÅ‚adowaÄ‡ te dane do bazy wektorowej. I przed tym musimy sobie powiedzieÄ‡ coÅ›, co siÄ™ nazywa Document Chunking. Chodzi o to, Å¼e mamy jakiÅ› okreÅ›lony rozmiar, ktÃ³ry jest sensownie w tej bazie danych przechowywaÄ‡. Czyli to bÄ™dzie np. jeden Chunk siÄ™ gdzieÅ› przyjmuje w taki defaultowy ostatnimi czasy, Å¼e jest to tysiÄ…c tokenÃ³w, ktÃ³re przechowujemy. To teraz bardzo upraszczam. Czyli mÃ³wimy sobie, Å¼e jest jeden Chunk, ktÃ³ry jest embedowany, to bÄ™dzie tysiÄ…c tokenÃ³w. Jak popatrzymy, chodzi o to, Å¼eby dokument, ktÃ³ry siÄ™ nie mieÅ›ci w tym zakresie podzieliÄ‡. Czyli pozwala nam przetwarzanie dÅ‚uÅ¼szych modeli niÅ¼ mamy, moÅ¼emy przechowaÄ‡, przetworzyÄ‡ przez model embedingowy, czy przetworzyÄ‡, wrzuciÄ‡ do naszego potem i wykorzystaÄ‡ to w naszym prompcie, ktÃ³ry wysyÅ‚amy do tego modelu. JeÅ¼eli popatrzymy, to to wszystko wymaga odpowiedniej strategii podziaÅ‚u tego tekstu. I najprostsze podziaÅ‚y to jest po prostu: weÅº mi to, potnij po liczbie tokenÃ³w, czyli nie patrz na kontekst i potnij nam to np. co tysiÄ…c tokenÃ³w. MogÄ… byÄ‡ bardziej rozbudowane strategie, ktÃ³re rozumiejÄ… w jakiÅ› sposÃ³b semantykÄ™. Np. to dobrze wyglÄ…da w stosie Microsoftowym i w Azure AI Searchu, ktÃ³ry jest bazÄ… wektorowÄ… rÃ³wnieÅ¼ i posiada te wszystkie integracje i mÃ³wi np. przetwarzajÄ…c z wykorzystaniem OCR-a dokument: zamieÅ„ mi np. ten dokument PDF-a na Markdowna i potnijmy to po strukturze paragrafÃ³w. ZostawiÄ™ Wam linka, bÄ™dziecie mogli zobaczyÄ‡, teÅ¼ to opisywaliÅ›my w poprzednich shortach, taki duÅ¼y sample jak jest ciÄ™ty dokument. Ale chodzi o to, Å¼eby go jak najbardziej inteligentnie pociÄ…Ä‡.

**Szymon Warda**: No i mamy jeszcze opcjÄ™ tak naprawdÄ™ chunk overlapping, czyli mÃ³wimy, Å¼e te chunki nie sÄ… zero-jedynkowe, tylko zachodzÄ… na siebie.

**Åukasz KaÅ‚uÅ¼ny**: Tak, ale do tego jeszcze pÃ³jdziemy potem, po paru innych pojÄ™ciach, jako takie zamkniÄ™cie pod klamrÄ… caÅ‚oÅ›ci. Dobra, jak popatrzycie sobie, to jak mÃ³wimy o tym podziale, to jest caÅ‚y retrieval pipeline, czyli Å‚adowania tych danych i wyciÄ…gania ich. I dzielimy go na dwie fazy. Jedna to faza Å‚adowania, preprocessingu i zaÅ‚adowania do bazy wektorowej, a druga jest faza inferencingu, czyli wykorzystania tego juÅ¼ faktycznie z LLM-em. I faza Å‚adowania, to bÄ™dzie po pierwsze, przetworzenie dokumentÃ³w, jak je wrzucacie na koniec dnia na tekst i podzielenie tego na Chunki. Czyli macie Wordy, PDF-y, Excele, cokolwiek i przerobienie tego na tekst i podzielenie. NastÄ™pnie dla kaÅ¼dego z tych ChunkÃ³w generujemy embeddingi wÅ‚aÅ›nie modelem embeddingowym, jakimÅ› API. I na koÅ„cu zapisujemy to w bazie danych jako wektor. I teraz bardzo waÅ¼ne z zapisem, w bazie danych zapisujemy kilka elementÃ³w. Czyli zapisujemy zazwyczaj skÄ…d pochodzi ten dokument, czyli jaka byÅ‚a nazwa dokumentu, nazwa pliku, kiedy. Zapisujemy sam Chunk, czyli ten oryginalny tekst, ktÃ³ry uÅ¼yliÅ›my i do tego embedding. W samplu Microsoftowym, ktÃ³ry Wam podrzuciliÅ›my, ktÃ³ry dobrze definiuje taki indeks, to tam np. sÄ… jeszcze kolumny semantyczne, czyli dokÅ‚adny nagÅ‚Ã³wek, np. nagÅ‚Ã³wek pierwszego poziomu, drugiego i trzeciego, z ktÃ³rego pochodzi tekst, Å¼eby daÄ‡ jeszcze wiÄ™cej kontekstu LLM-owi, do ktÃ³rego to wrzucimy, ale teÅ¼ czÅ‚owiekowi, Å¼eby powiedzieÄ‡ skÄ…d to pochodziÅ‚o, Å¼eby zobaczyÄ‡ i trochÄ™ nadaÄ‡ rozliczalnoÅ›ci temu modelowi, Å¼eby zrozumieÄ‡ skÄ…d on w ogÃ³le wziÄ…Å‚ tÄ… odpowiedÅº. I teraz jak mÃ³wiÄ™ o wykorzystaniu, tu bÄ™dzie clue caÅ‚oÅ›ci. Jak Wy wysyÅ‚acie zapytanie i ten RAG mieli to pod spodem, to to co siÄ™ dzieje tak naprawdÄ™, to wasze pytanie jest przerabiane rÃ³wnieÅ¼ na embeddding, wysyÅ‚ane w postaci wektora do bazy wektorowej i baza wektorowa korzystajÄ…c z tego, co Szymon powiedziaÅ‚, o przeszukiwaniu, similarity searchu, wyciÄ…ga te wasze odpowiedzi. I teraz magia skÄ…d LLM pracuje na naszych danych polega na tym, Å¼e Wasza integracja doÅ‚Ä…cza te dane, zwrÃ³cone Chunki, doÅ‚Ä…cza po prostu jako kolejne elementy prompta, ktÃ³ry wysyÅ‚acie do LLM-a. I na tym koÅ„czy siÄ™ caÅ‚a magia pod tytuÅ‚em Bring Your Own Data.

**Szymon Warda**: Jest to doÅ›Ä‡ proste. Dobrze, Åukaszu powiedziaÅ‚eÅ›, Å¼e szukamy podobieÅ„stwa po wektorach, co nie jest do koÅ„ca prawdÄ…, bo mamy takie rzeczy jak Hybrid Search. Czyli czym to jest? No bo szukanie po wektorach ma swoje plusy, bez dwÃ³ch zdaÅ„. Zdecydowanie jest faktycznie fajne, dziaÅ‚a moÅ¼e na pewne rzeczy. Jednak takie zwykÅ‚e full textowe wyszukiwanie teÅ¼, szczegÃ³lnie z tak bardzo rozbudowanym pipeline'em jaki moÅ¼emy mieÄ‡, teÅ¼ ma swoje wartoÅ›ci. WiÄ™c teraz ktÃ³rÄ… wersjÄ™ wybraÄ‡? Ano nie wybierajmy Å¼adnej, tylko poÅ‚Ä…czmy. W tym momencie co my robimy? Robimy to, Å¼e model otrzymujÄ…c zapytanie odpala dwa wyszukiwania, jedno wÅ‚aÅ›nie wektorowe, a drugie wÅ‚aÅ›nie peÅ‚notekstowe. I Å‚Ä…czÄ…c dane z tych dwÃ³ch zapytaÅ„, idÄ™ duplikujÄ…c to oczywiÅ›cie, dopiero to idzie jako to wejÅ›cie i jest doÅ‚Ä…czane wÅ‚aÅ›nie do inputu, poniewaÅ¼ jak juÅ¼ powiedzieliÅ›my modele sÄ… bezstanowe.

**Åukasz KaÅ‚uÅ¼ny**: Tak.

**Szymon Warda**: JeÅ¼eli teraz wchodzimy w ogÃ³le w ten obszar wÅ‚aÅ›nie takiego Full-Text Searcha i wchodzimy w obszar dokumentÃ³w i wyciÄ…gania, to trzeba powiedzieÄ‡ o jednej rzeczy. Dokument dokumentowi rÃ³wny nie jest, o tym doskonale wiemy. Ktokolwiek kto bawiÅ‚ siÄ™ Full-Text Searchem albo nawet wyszukiwaniami Google'a, ktÃ³re teÅ¼ sÄ… formÄ… w Full-Text Searcha, doskonale wie, Å¼e np. wyniki z Wikipedii sÄ… generalnie bardziej wiarygodne niÅ¼ np. z komentarzy na Onecie czy gdziekolwiek indziej, albo na Facebooku. I o co chodzi? Chodzi teraz o to, Å¼e chcielibyÅ›my jakoÅ› odzwierciedliÄ‡ tÄ… wiarygodnoÅ›Ä‡ dokumentÃ³w tak naprawdÄ™. Bo jeÅ¼eli dokumenty sÄ… zwracane, one sÄ… zwracane w jakiejÅ› kolejnoÅ›ci. I co to jest za kolejnoÅ›Ä‡? A to jest kolejnoÅ›Ä‡ taka lucynowa, taka semantyczna, czyli jak waÅ¼ne byÅ‚o to sÅ‚owo w kolejnym dokumencie, jak to sÅ‚owo byÅ‚o waÅ¼ne w caÅ‚ym naszym zbiorze, itd. I to co robimy przez reranking to jest to, Å¼e do pewnych dokumentÃ³w, do pewnych ÅºrÃ³deÅ‚ przypisujemy wyÅ¼szÄ… wagÄ™ znaczenia, czyli finetune'ujemy nasz zbiÃ³r danych i jak waÅ¼ny on jest. Czyli informujemy nasz model, naszego search'a o tym, co podjÄ…Ä‡ jakieÅ› wagi. DziÄ™ki temu uzyskujemy taki element, Å¼e nasz model wie o wiarygodnoÅ›ci danych zbiorÃ³w, co odbija siÄ™ oczywiÅ›cie teÅ¼ na jakoÅ›ci odpowiedzi i odbiera siÄ™ na to, Å¼e moÅ¼emy bardzo Å‚adnie dostosowaÄ‡ jak on siÄ™ zachowa i na co bÄ™dzie bardziej patrzyÅ‚.

**Åukasz KaÅ‚uÅ¼ny**: Tak i jak mÃ³wimy o tuningu to pojawiajÄ… siÄ™ wÅ‚aÅ›nie chunk overlapping, o ktÃ³rym wspomniaÅ‚eÅ›. Czyli bardzo prosto, Chunki muszÄ… na siebie nachodziÄ‡. Czyli np. Å¼e tekst jest zdeduplikowany po to, Å¼eby jeÅ¼eli szukamy jakiejÅ› frazy, jakiejÅ› odpowiedzi na pytanie, Å¼eby zwrÃ³ciÄ‡ jak najwiÄ™cej ChunkÃ³w, ktÃ³re bÄ™dÄ… miaÅ‚y wiarygodny kontekst i bÄ™dÄ… ze sobÄ… Å‚Ä…czyÅ‚y ten kontekst. Czyli mÃ³wimy: weÅº mi zrzuÄ‡ sÄ…siadujÄ…ce fragmenty i zrÃ³b zakÅ‚adkÄ™ po 20% np., po 20, 25% w Chunkach, Å¼eby zwrÃ³ciÄ‡ jak najlepszÄ… jakoÅ›Ä‡ poprzez wÅ‚aÅ›nie rerankingi, hybrid searche, Å¼eby to zÅ‚oÅ¼yÄ‡. I potem na bazie tego, takÄ… ostatniÄ… technikÄ…, najbardziej wspomaga... Inaczej, to jest juÅ¼ takie creme de la creme po chunk overlappingu, ktÃ³ry powinien byÄ‡ stosowany w ogÃ³le... Powiedzmy sobie wprost, chunk overlapping w jakiejÅ› formie powinien byÄ‡ stosowany jako default. Ja np. siÄ™ cieszÄ™, Å¼e wiele teraz z tych rzeczy juÅ¼ to robi automatycznie i domyÅ›lnie, to jest query expansion. I to jest technika, ktÃ³ra przerabia nam oryginalne zapytanie na kilka rÃ³Å¼nych przykÅ‚adÃ³w, czyli pozwala znaleÅºÄ‡ lepiej i lepiej odpytaÄ‡. I teraz to bÄ™dzie bardzo dziecinny prompt, bo bÄ™dzie on, normalnie stosuje siÄ™ bardziej zaawansowane, ale brzmi on nastÄ™pujÄ…co: wygeneruj mi trzy alternatywne wersje zapytania zachowujÄ…c jego znaczenie. Czyli polega to na tym, Å¼e zanim zaczniemy w ogÃ³le generowaÄ‡ embedding i przeszukiwaÄ‡ naszego RAG-a, tak naprawdÄ™ korzystajÄ…c z jakiegoÅ› mniejszego modelu np. GPT-4o mini czy od Anthropica Haiku. Przerabiamy pytanie uÅ¼ytkownika, Å¼eby wyszukaÄ‡ i dopiero wysyÅ‚amy do ciÄ™Å¼szego modelu te gotowe, wynalezione w ten sposÃ³b dane.

**Szymon Warda**: Dobra, to co? Teraz idziemy jak to wszystko zintegrowaÄ‡?

**Åukasz KaÅ‚uÅ¼ny**: Tak, raczej jak dodaÄ‡ jeszcze wiÄ™cej inteligencji.

**Szymon Warda**: Tak, to znaczy jak zintegrowaÄ‡. To teraz pierwszy, najprostszy, tzw. function calling. O co z tym chodzi? A no bÄ™dÄ… takie sytuacje, kiedy nie wszystko w naszego RAG-a wpakujemy albo inny przypadek, Å¼e dane i nawet gdybyÅ›my to w RAG-a wrzucili, bÄ™dÄ… nieaktualne. Czyli mamy zbiÃ³r danych, ktÃ³ry jest na tyle zmienny, Å¼e wrzucanie tego do RAG-a po prostu nie ma wiÄ™kszego sensu. WiÄ™c co moÅ¼emy zrobiÄ‡? MoÅ¼emy wykorzystaÄ‡ wÅ‚aÅ›nie function calling, czyli poinformowaÄ‡ nasz model, Å¼e: ej, mamy taki zbiÃ³r danych, ale nie przeszukuj go przez RAG-a, czyli nie w ten sposÃ³b, tylko masz sposÃ³b na odpytanie siÄ™ tego zbioru jak co dostaÄ‡. Dajemy takÄ… funkcjÄ™ i umoÅ¼liwiamy naszemu modelowi pobranie danych z innych ÅºrÃ³deÅ‚ niÅ¼ te najbardziej domyÅ›lne.

**Åukasz KaÅ‚uÅ¼ny**: I z drugiej strony teÅ¼ warto dodaÄ‡, moÅ¼e on rÃ³wnieÅ¼ wykorzystywaÄ‡ akcje np.: dodaj mi zadanie do Todoista, czy zrÃ³b wpis w kalendarzu, czy przygotuj draft maila.

**Szymon Warda**: Tak, faktycznie to nie sÄ… tylko rzeczy odczytowe, mogÄ… to teÅ¼ byÄ‡ akcje, czyli Å¼e coÅ› wykonujemy tak naprawdÄ™ w formie czynnoÅ›ci.

**Åukasz KaÅ‚uÅ¼ny**: Czyli to peÅ‚en CRUD, moÅ¼na podpiÄ…Ä‡ peÅ‚nego CRUD-a pod to. JeÅ¼eli patrzymy, no to mÃ³wimy teraz o integracji juÅ¼ w aplikacji, to przydaÅ‚oby siÄ™ nam nie dostawaÄ‡ tekstu, tylko jakiÅ› ustrukturyzowany output. I w modelach pojawiÅ‚o siÄ™ coÅ› takiego jak wsparcie dla structure output, czyli technika, ktÃ³ra wymusza nam, 99% przypadkÃ³w, to bÄ™dzie JSON, czyli Å¼eby odpowiedziaÅ‚ nam w jakimÅ› konkretnym schemacie i pozwoliÅ‚ nam przygotowaÄ‡ ten output, Å¼eby byÅ‚ w okreÅ›lonym schemacie, czyli wynik prompta ma np. byÄ‡ zawsze JSON-em, w tym schemacie w te pola upchnij to. I to bÄ™dzie w ogÃ³le clue potem caÅ‚ych function calling i structure output, to jest caÅ‚e clue Waszych integracji, ktÃ³re bÄ™dziecie wykonywaÄ‡.

**Szymon Warda**: Dobra, to jak juÅ¼ mÃ³wimy o integracjach, to trzeba powiedzieÄ‡ jeszcze o jednej bardzo waÅ¼nej rzeczy, o strumieniowaniu odpowiedzi. No bo do otrzymania peÅ‚nej odpowiedzi od naszego modelu to bÄ™dzie tak 20-30 sekund. Wy tego moÅ¼e nie czujecie tak naprawdÄ™, ale jak nawet macie tego chata, bo pewnie wiÄ™kszoÅ›Ä‡ z was miaÅ‚o z nim do czynienia, odpowiedzi Å‚adnie sobie ingeruje kawaÅ‚ek po kawaÅ‚ku i to nie wydaje siÄ™, Å¼eby tak dÅ‚ugo trwaÅ‚o. A jakbyÅ›cie tak zatrzymali siÄ™ i pytanie, po czym chwila przerwy, taka dÅ‚uÅ¼sza chwila, powiedzmy te 10 sekund i nagle dopiero odpowiedÅº, to nie dziaÅ‚aÅ‚oby najlepiej mÃ³wiÄ…c bardzo prosto. A skoro wiemy, Å¼e te modele i tak pod spodem generujÄ… kawaÅ‚ek po kawaÅ‚ku, to wÅ‚aÅ›nie bardzo waÅ¼ne jest brzmienie tych odpowiedzi, czyli dziaÅ‚anie tak jak chat, czyli Å¼e Å‚adnie widzimy co siÄ™ dzieje, itd. To ma jeszcze kolejny plus, Å¼e dziÄ™ki temu strumieniowaniu uÅ¼ytkownik ma moÅ¼liwoÅ›Ä‡ przerwania dalszego generowania jeÅ¼eli widzi, Å¼e odpowiedÅº jest w ogÃ³le nie na temat, nie o to mu chodziÅ‚o, wiÄ™c moÅ¼emy teÅ¼ trochÄ™ zaoszczÄ™dziÄ‡ funduszy jeÅ¼eli chodzi o samo dziaÅ‚anie, ale przede wszystkim to jest dobry user experience wÅ‚aÅ›nie, Å¼eby po kawaÅ‚ku to generowaÄ‡.

**Åukasz KaÅ‚uÅ¼ny**: Tak, jak sobie pÃ³jdziemy dalej to pojawia siÄ™, bo tych function callingÃ³w, outputÃ³w moÅ¼emy mieÄ‡ duÅ¼o. JednÄ… z takich technik jest sobie skill orchestration, czyli Å¼e tak budujemy system prompt, Å¼e ma nam powiedzieÄ‡, Å¼e zrÃ³b XYZ, to wykorzystaj to. MoÅ¼e rzucajÄ…c przykÅ‚adem to bÄ™dzie: przeanalizuj opinie klientÃ³w z rÃ³Å¼nych krajÃ³w i przygotuj raport. Czyli jednÄ… z takich rzeczy bÄ™dzie: przetÅ‚umacz mi te wszystkie opinie na jeden jÄ™zyk, np. na polski czy na angielski, potem dokonaj oceny sentymentu, nastÄ™pnie kategoryzacji, generuj raport i to wszystko np. wyrzuÄ‡ nam w JSON-ie. Czyli krok po kroku, w zaleÅ¼noÅ›ci co tam wpadnie, tÅ‚umaczymy co on ma z tym robiÄ‡. To co siÄ™ pojawiÅ‚o z myÅ›lÄ… dalej, to sÄ… agenci AI, AI Agents. Czyli maÅ‚e kawaÅ‚ki aplikacji, promptÃ³w, ktÃ³re majÄ… wÅ‚aÅ›nie byÄ‡ moÅ¼e swojego RAG-a, swÃ³j function calling, w ktÃ³rym majÄ… jedno wyspecjalizowane zadanie. Czyli one majÄ… automatycznie nam coÅ› przetwarzaÄ‡, np. odczytywaÄ‡ z systemu, dostarczaÄ‡ wiedzÄ™. I potem zbiÃ³r takich agentÃ³w moÅ¼emy uÅ‚oÅ¼yÄ‡ w jednÄ… wiÄ™kszÄ… caÅ‚oÅ›Ä‡. MoÅ¼emy pomyÅ›leÄ‡ o systemie wsparcia klienta w banku, Å¼eby nie zÅ‚oÅ¼yÅ‚ reklamacji. Czyli np. czemu mam opÅ‚atÄ™, albo w telefonii komÃ³rkowej, czemu mam opÅ‚aty na koncie albo czemu mÃ³j rachunek jest wyÅ¼szy niÅ¼ zazwyczaj. I jeden agent moÅ¼e odpowiadaÄ‡ za pobieranie danych o kliencie, drugi agent moÅ¼e siÄ™ Å‚Ä…czyÄ‡ do systemu billingowego, trzeci po zebraniu tych danych moÅ¼e napisaÄ‡ takÄ… odpowiedÅº i przekazaÄ‡ klientowi. Czyli budujemy zbiÃ³r takich maÅ‚ych grupek mrÃ³wek, ktÃ³re potem Å‚Ä…czÄ… to w jednÄ… caÅ‚oÅ›Ä‡.

**Szymon Warda**: DokÅ‚adnie. Dobrze Åukaszu, lista zakoÅ„czona. I co? Tyle. W sumie teraz tylko czekamy na wasz feedback czego zapomnieliÅ›my, czego brakuje, gdzie siÄ™ pomyliliÅ›my, itd., bo pewnie siÄ™ coÅ› pojawi, ale to nawet dobrze.

**Åukasz KaÅ‚uÅ¼ny**: Tak, dokÅ‚adnie. To byÅ‚a taka nasza lista, jak zobaczycie te podstawowe pojÄ™cia z promptami, techniki promptowania, RAG-i, czy potem te integracje, ktÃ³re, jeÅ¼eli zaczynacie z LLM-ami, to pozwolÄ… Wam zrozumieÄ‡ i zobaczycie, Å¼e wynika to po prostu z praktyki jakich pojÄ™Ä‡ najczÄ™Å›ciej bÄ™dziecie wykorzystywaÄ‡.

**Szymon Warda**: DokÅ‚adnie. Dobrze.

**Åukasz KaÅ‚uÅ¼ny**: Trzymajcie siÄ™.

**Szymon Warda**: Tyle.

**Åukasz KaÅ‚uÅ¼ny**: Hej.

**Szymon Warda**: Hej!

