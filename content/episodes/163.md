---
title: '#163 Prosto i praktycznie wyjaniamy: MCP (Model Context Protocol) '
date: 2025-09-26T08:00:00+02:00
episode: "163"
tags: ["Prosto i praktycznie", "MCP", "AI", "LLM", "Architecture"]
description: "MCP (Model Context Protocol) wyjaniony bez szamastwa. Dlaczego Jira MCP jest zym przykadem, jak bezpieczestwo le偶y i kwicze i czemu za du偶o narzdzi zabija LLM?"
seo_keywords: "MCP, Model Context Protocol, LLM, function calling, anthropic, ai agents, narzdzia ai, llm tools, architektura ai, ai integration, protok贸 ai, claude"

# Hugo fields
youtube_id: "yrNf0_dxVYQ"
youtube_url: "https://www.youtube.com/embed/yrNf0_dxVYQ?enablejsapi=1"

# Social media images (poprawione nazwy)
og_landscape: "/img/163-landscape.webp"
og_square: "/img/163-square.webp"

# Intro for episode
intro: |
  **"Wicej ludzi buduje MCP serwery ni偶 ich faktycznie u偶ywa"** - mem, kt贸ry trafi w sedno przed nagraniem. ukasz i Szymon rozwiewaj hype wok贸 **Model Context Protocol** i tumacz, dlaczego wikszo implementacji to bezu偶yteczny kod.
  
  **MCP od Jiry** zwraca surowe API "as is" - _"wyrzuca  nieudokumentowanych p贸l i LLM  z tego nie rozumie"_. **Stripe MCP** trzyma token admina w plain texcie. A ty mylisz, 偶e **JSON RPC** na **stdio** to przyszo? 
  
  Prawda jest brutalna: **MCP to API Gateway dla LLM-贸w**, nie kolejny CRUD. Jeli nie przemapujesz logiki biznesowej, model zacznie **halucynowa** na podstawie samych tytu贸w. Jak si okazuje, _"autonomiczny agent"_ to tylko cron job z problemami z **prompt injection**.
  
  Od **vibe coding** w **Cursorze** po korporacyjne **OAuth** i **Azure API Gateway** do zabezpieczania. Dowiesz si, czemu **human-in-the-loop** to nie opcja, tylko konieczno, zanim agent zleakuje wszystkie sekrety przez **GitHub issue**.
  
  Przesta myle o MCP jak o magicznym rozwizaniu. To protok贸, kt贸ry wymaga mylenia - co, czego brakuje w 90% implementacji na GitHubie.
  

# Links for the episode
links:
  - title: "OpenAI Platform"
    url: "https://platform.openai.com/docs/mcp"
  - title: "MCP Toolkit"
    url: "https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit"
  - title: "Sampling"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/client/sampling"
  - title: "M贸j setup MCP - realnie dziaajcy w 2025"
    url: "https://kaluzny.io/moj-setup-mcp-realnie-dzialajacy-w-2025"
  - title: "GitHub - modelcontextprotocol/inspector: Visual testing tool for MCP servers"
    url: "https://github.com/modelcontextprotocol/inspector"
  - title: "Example Servers - Model Context Protocol"
    url: "https://modelcontextprotocol.io/examples"
  - title: "Roots - Model Context Protocol"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/client/roots"
  - title: "Overview - Model Context Protocol"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/basic"
  - title: "Specification - Model Context Protocol"
    url: "https://modelcontextprotocol.io/specification/2025-06-18"
  - title: "OWASP Top 10 for Large Language Model Applications | OWASP Foundation"
    url: "https://owasp.org/www-project-top-10-for-large-language-model-applications"
  - title: "Prompts"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/server/prompts"
  - title: "Elicitation"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation"
  - title: "Resources"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/server/resources"
  - title: "Tools"
    url: "https://modelcontextprotocol.io/specification/2025-06-18/server/tools"
  - title: "Set up Standard Workflows as MCP Servers - Azure Logic Apps"
    url: "https://learn.microsoft.com/en-us/azure/logic-apps/set-up-model-context-protocol-server-standard"
  - title: "Critical flaw in Microsoft Copilot could have allowed zero-click attack"
    url: "https://www.cybersecuritydive.com/news/flaw-microsoft-copilot-zero-click-attack/750456"
  - title: "Claude - Model Context Protocol (MCP)"
    url: "https://docs.claude.com/en/docs/mcp"
---

**ukasz Kau偶ny**: Je偶eli co jest niepotrzebne, to nie powinno by wystawiane. Wic o MCP mo偶na pomyle jako API Gateway'u, ale to nie jest API Gateway w tym rozumieniu, w kt贸rym go u偶ywalimy. Pierwsza rzecz, o kt贸rej powiem, za du偶o narzdzi, za du偶o zwracamy i przez to mamy albo wyczerpanie okna kontekstowego w LLM-ie, albo problemy z trzymaniem si, bo jest za du偶o kontekstu.

**Szymon Warda**: Warto by leniwym. Tak, zdecydowanie, bo te opisy, kt贸re s generowane, s generowane naprawd dobrze i s generowane w taki spos贸b, w jaki LLM to adnie zrozumie.

**ukasz Kau偶ny**: Dobra, tu jest du偶y problem i podam to na przykadzie MCP od Jiry. Cze, suchacie Patoarchitekt贸w. Prowadz ukasz Kau偶ny...

**Szymon Warda**: I Szymon Warda. Wszystkie linki do tego odcinka o dziwo, jak zwykle Patoarchitekci.io. Wygooglacie, wyczatujecie, znacie. Dobrze. Dzisiaj fajny odcinek, bo dzisiaj znowu odcinek merytoryczny bdzie. O czym bdziemy, bo dzisiaj dyskutowiec.

**ukasz Kau偶ny**: Dobra, Model Context Protocol, czyli przejdziemy merytorycznie, wytumaczymy czym jest bez zbdnego szamastwa i mem贸w, kt贸re mogy by. Dzisiaj przed tym jak nagrywamy by wietny mem, kt贸ry jak widzicie, zaraz go zobaczycie na ekranie, wicej jest os贸b budujcych MCP Servery ni偶 ich faktycznych u偶ytkownik贸w.

**Szymon Warda**: Tak samo jak z pakietami r贸偶nymi do node'a i tak dalej. Teraz jest hype. Dobrze ukaszu, 偶eby tak to uproci tak naprawd, to czym jest waciwie MCP? Jakby mia tumaczy czterolatkowi.

**ukasz Kau偶ny**: Dobra, MCP, mo偶e nie czterolatkowi, nie dziesiciolatkowi, to jest dynamiczne rozszerzenie function calling, kt贸re mamy w LLM-ach. Czyli LLM mo偶e nam, w zale偶noci jak tam model jest zbudowany, mo偶e nam zwr贸ci odpowied藕, 偶e prosi o wywoanie jakiego narzdzia z parametrami i my w swoim kodzie to mo偶emy obsu偶y i wywoa jakie API Restowe. A...

**Szymon Warda**: Nie tylko narzdzia. Mo偶e te偶 dopyta si o co nam waciwie chodzi.

**ukasz Kau偶ny**: Tak, ale tutaj function calling to s narzdzia, czyli odpalenie czego konkretnego u nas i to obsugujemy ze swojej strony kodu. Mo偶emy wzi, tak jak wszyscy, w samplach zrobi kalkulator albo odpyta si dynamicznie od pogody jakiego Restowego API. A tutaj chodzio o to, 偶eby zrobi decoupling, odseparowa function calling od naszego kodu aplikacji agenta i pozwoli na uniwersalne u偶ywanie tego. I w istocie MCP je偶eli popatrzymy, to jest wystandaryzowany protok贸, kt贸ry pozwala nam dostarczy nakadk na function calling, jak na to popatrzymy, 偶eby dynamicznie wykrywa narzdzia i zasoby, kt贸re s do wykorzystania i potem je wywoywa. Ja bym tak na to popatrzy. Czyli 偶e to jest otwarty protok贸.

**Szymon Warda**: Czyli upraszczajc, to jest protok贸, kt贸ry umo偶liwia nam dwojako. To, 偶eby model dowiedzia si, co waciwie klient mo偶e zrobi, a 偶eby klient dowiedzia si, co waciwie model mo偶e zrobi i 偶eby nawzajem mogli wywoa, i konkretne wywoania obsugiwa.

**ukasz Kau偶ny**: I to jest tak, to jest istotny element. Jak tam popatrzymy historycznie, zao偶enie jest, ma prawie rok, wic jest jak wszystkie rzeczy wie偶ynka. Zosta zapocztkowany przez Anthropica i wszyscy duzi od modeli do tego kontrybuuj. I stao si takim wanie, jak mo偶na ju偶 popatrze na ten model, powiedzie wprost, to jest standard dostarczania narzdzi do LLM-贸w, je偶eli budujemy agenta, asystenta, jaki kod AI-owy. Tak na to popatrzmy.

**Szymon Warda**: Jeszcze co wa偶ne, bo to m贸wi, 偶e to powstao rok temu. W zupenoci zgadzamy si, ale to nie standard, kt贸ry powsta rok temu i le偶y. On si cay czas tam, drobne rozwoje s, wersje wychodz i tak dalej.

**ukasz Kau偶ny**: Tak, to jest, cay czas si rozwija. Raczej zbli偶a si do jakiej stabilnoci. O tak, to mo偶emy adnie okreli, 偶e zaczyna wchodzi w faz stabilnoci, je偶eli mo偶na nazwa rok czasu rozwoju produktu, standardu.

**Szymon Warda**: Je偶eli teraz m贸wimy o standardzie, to w takim razie musimy dowiedzie si, co waciwie ten standard standaryzuje.

**ukasz Kau偶ny**: Czyli architektura. I to jest MCP Client. Czyli jak nasz kod LLM, jak mo偶emy do LLM-a podczy to MCP i potem MCP Server, czyli co, co ju偶 faktycznie wykonuje t akcj i nam dostarcza. Czyli tam le偶y ta caa integracja, kt贸ra jest potrzebna. I w sumie to jest chyba najwa偶niejsze do powiedzenia.

**Szymon Warda**: ... nazywasz. Dobra, to co jeszcze mamy? Mam jeszcze transporter.

**ukasz Kau偶ny**: Dobra, cao jest zbudowana na starym dobrym JSON RPC pod spodem, czyli komunikacja leci, cao standaryzowana jest w JSON RPC. Czy jest to dobre czy ze? Mo偶na dyskutowa. Uwa偶am, 偶e jest good enough, o tak. I potem mamy transport, czyli pierwsze dla lokalnych rzeczy jak odpalamy, to jest po prostu standard IO, czyli po prostu komunikacja przez konsol. I pozostawmy to bez komentarza, potem jeszcze do tego przejdziemy. A drugie, to jest Streamabel HTTP, czyli po prostu po HTTP2.

**Szymon Warda**: Do negocjacji protokou. Jest to opisane do dokadnie w dokumentacji, tego ju偶 tu nie bdziemy wchodzi.

**ukasz Kau偶ny**: Wa偶ne jest, 偶e s negocjacje co dostarcza serwer. Czyli je偶eli mamy klienta z dowolnego SDK, to on mo偶e odpyta serwer w jakiej wersji dziaa i co on dostarcza. I to jest dobry element.

**Szymon Warda**: Protok贸, kt贸ry jest taki, wida, 偶e jest przemylany pod ktem tego, 偶e te serwery powstan i mo偶e nie bd rozwijane. Wic 偶eby byy negocjacje, co klient umie obsugiwa, co serwer mo偶e obsugiwa i tak dalej. Cakiem niezy mechanizm w fallbacku tam wpisane, jest to ustandaryzowane. Oczywicie, 偶ebymy si suchali tego jak to powinno by zrobione. Dobra, to teraz idziemy. Ok, wiemy co mamy waciwie je偶eli chodzi co to robi. To teraz pytanie kolejne, do czego bdziemy u偶ywali?

**ukasz Kau偶ny**: I rozdzielmy wiat, bo powiedziabym mamy szaman贸w z AI-a, kt贸rzy m贸wi, influencer贸w czy ludzi podajcych si za programist贸w, a tak naprawd nie potraficych kodowa, bo robi to tylko dziki vibe codingowi. I trzeba rozdzieli dwa elementy, teraz ju偶 na powa偶nie. Pierwszy scenariusz, kt贸ry jest szerzej omawiany cigle, to jest lokalna produktywno. Czyli do mojego Claude chata, VSCode'a, Cursora, Claude Code'a czy tam innych narzdzi, dostarczam wanie r贸偶ne wystandaryzowane narzdzia, kt贸re s zazwyczaj lokalnie odpalane, ale mo偶e te偶 zdalnie. Ja zostawiam tutaj tak list z tego, co ja na przykad u siebie u偶ywam. Jednym z takich przykad贸w chyba, kt贸re mog poleci, to jest Context 7. I jak si opisuj: up to date documentation for LLM's and AI code editors. Czyli 偶eby LLM m贸g sobie cign aktualn dokumentacj do jakiej biblioteki.

**Szymon Warda**: Bo to jest w og贸le, jak ju偶 zatrzymalimy si przy tym, bardziej ruszylimy, to jest bardzo wa偶ne, 偶eby nakierowa jak to wykorzystanie wanie MCP wyglda, szczeg贸lnie lokalnie. Bo to jest tak, 偶e stawiamy sobie czsto bardzo mae, drobne serwerki MCP, czyli rzeczy, kt贸re programy waciwie, kt贸re umo偶liwiaj nam jakie dodatkowe mo偶liwoci, kt贸re mo偶e LLM wykorzysta i to nam umo偶liwia dostp do plik贸w, kalendarza, jak Ty m贸wie wanie do dokumentacji i tak dalej. Bo idea jest taka, 偶e LLM nie bdzie tego wykonywa, nie bdzie siga, on jest tylko tym statystycznym mechanizmem, kt贸ry wylicza nam to wszystko, ale dziki temu wanie dajemy dodatkowe mo偶liwoci, mo偶emy go adnie rozszerzy. I to jest wanie ten cay clue tego gdzie MCP bdzie u偶ywane.

**ukasz Kau偶ny**: Tak i dobrze, 偶e wspomniae lokalnie, bo zazwyczaj te lokalnie oznacza trzy cie偶ki, jak odpalamy lokalnie. Pierwsza cie偶ka to jest odpal co NPX-em, czyli kawaek source kodu node'owego, albo zacignij paczk i j odpal i dosta si do niej przez stdio. Druga, to odpal, zainstaluj co z pip'a, cignij jak paczk pip'ow. Trzecia, taka kulturalna to nazwijmy, odpal Dockera. To s trzy takie rzeczy, kt贸re si najczciej z tym kodem dziej i 99% przypadk贸w to jest odpalanie stdio.

**Szymon Warda**: Czasami pytamy si jaki model, czy sobie chatujemy z nim, 偶eby co zrobi, to on nam m贸wi: ok, ja tego nie umiem zrobi, ale umiem napisa program w Pythonie, kt贸ry to obliczy, uruchomi, zwr贸ci mi wynik. I to wanie ten jest moment, kiedy na tym MCP ..., bo znaczy dajemy mo偶liwo uruchomienia jakiego Pythona, dajemy mo偶liwo dostpu do plik贸w. Oczywicie polecamy limitowa generalnie do kt贸rych katalog贸w maj, bo to mo偶e skoczy si 藕le, ale o tym jeszcze powiemy sobie. Tak 偶e tak, to daje naprawd bardzo du偶o mo偶liwoci. To nie jest tylko w tym momencie pytanie - odpowied藕.

**ukasz Kau偶ny**: I z takich element贸w jeszcze tylko dorzuc, 偶e mo偶na si zastanowi, bo troszeczk przy tym stdio inaczej to dziaa. Jak ju偶 jestemy przy dockerach, ja zostawi jeszcze Wam linka, na przykad taki Docker Desktop, z czego ja tam gdzie z boku te偶 troch korzystam, wrzuci sobie lokalnie MCP catalog, 偶eby odpala to w dockerach. Taka ciekawa ewentualno i wtedy do LLM-a podpinamy tak jeden MCP Server, kt贸rym jest wanie ten gateway na standard IO, a reszta przelatuje mu dynamicznie z tego co wrzucilimy w nasz konfiguracj Docker Desktopa.

**Szymon Warda**: Teraz nieco podnosimy temperatury szamastwa, nie za bardzo, ale odrobin i przechodzimy do ustawienia korporacyjne.

**ukasz Kau偶ny**: Dobra i korpo zastosowania, to s r贸偶nego rodzaju dostarczenie narzdzi w spos贸b wystandaryzowany. Bo MCP z zao偶enia ma da nam dostp do danych, jakie akcje i inne rzeczy. I chodzi tutaj prosto, 偶e raczej ju偶 w formie zdalnej, czyli wykorzystujc protok贸 po HTTP wystawiamy w firmie jakie narzdzia, kt贸re mog by wykorzystywane z Copilotem. Jeden przykad ze stosu Microsoftu, Copilot Studio, czyli te boty do Teams'贸w, kt贸re s low code'owe, nocode'owe, mo偶na dostarczy do nich akcj wanie w postaci takiego MCP zdalnego. Czyli podczamy si do serwera i infrastruktura na przykad Microsoftu azure'owa, teraz Logic Appsy mog by wystawiane jako MCP, Azure API Gateway pozwala Rest API zamieni w MCP, czyli mo偶emy to wystawi, takie akcje, plus inne cloudy te偶 z tego korzystaj. I teraz co jest wa偶ne do powiedzenia? Raz takie wystawione rozwizanie na takim API Gateway'u, Logic Appsach czy innym open source, jak na to popatrzymy i zbudowane takie integracje, pozwalaj nam wykorzysta to MCP przez wiele r贸偶nych zastosowa. Czyli mo偶e wykorzysta wielu r贸偶nych agent贸w, na przykad kt贸rych sobie napiszemy, mo偶e z tego skorzysta.

**Szymon Warda**: To jest jeszcze jedna rzecz, wa偶na rzecz, o kt贸rej nie powiedzielimy, ale generalnie m贸wimy, 偶e mo偶e wykorzysta. To pytanie powstaje, w jaki spos贸b s serwery MCP wykrywane, deklarowane i tak dalej? Musimy jej jawnie powiedzie, modelowi, 偶e: ok, to jest serwer MCP, on ma takie mo偶liwoci, tak, mo偶esz z nich korzysta i to adnie opisujemy, tak 偶eby rozwia wtpliwoci tego i jak ta magia dziaa. Bo tam magii w sumie nie ma do koca.

**ukasz Kau偶ny**: I wiesz co, chyba to jest dobry moment, 偶eby sobie powiedzie, co tak naprawd wystawia MCP, jego podstawowe elementy, te primitives, kt贸re s wystawione.

**Szymon Warda**: Taki schemat, mo偶na powiedzie.

**ukasz Kau偶ny**: Dobra, i pierwszym to jest, chyba bdzie najbardziej popularne, reszta jest zbdna, je偶eli popatrzymy na to, co si dzieje, to s toolsy, czyli narzdzia. I LLM sobie robi tam toollist, ma toollist, czyli sprawdzenie, klient sobie pobiera ca list i mamy potem toolscall, czyli wywoania. I teraz jak sobie popatrzymy, to jest po prostu function calling, czyli to co dostaje LLM, to jest function calling. I tu wchodzimy w cao, w pikno jak i zo. Modele LLM maj swoj dugo kontekstu, kt贸ry obsuguj. Powy偶ej pewnych, mimo, 偶e mo偶emy wprowadzi nie wiadomo ile, to si gubi, 藕le wybieraj narzdzia. Poniewa偶 ten...

**Szymon Warda**: ... wic gdzie go ukryjemy, albo w toolsach, albo w naszych promptach, albo w historii i tak dalej, 偶e gdzie wsadzimy, to gdzie bdziemy mieli mniej.

**ukasz Kau偶ny**: Tak. Im wicej wsadzimy. I teraz o co chodzi? Toole wystawiaj nam, pokazuj si do LLM-a po prostu jako jaki format JSON-a, jakbymy sobie popatrzyli w jaki spos贸b jest to wysyane pod spodem. I w tych toolach, to co znajdziemy, to jest nazwa toola, tam jego ID, ale to pomijamy teraz, nazwa toola, description i input z SCIM-a w JSON-ie. Czyli pierwsze, jak si nazywa narzdzie, opis do czego su偶y i jaki ma input. I na podstawie tego LLM bdzie musia podj decyzj, je偶eli mu jawnie nie powiemy, 偶e skorzystaj z tego narzdzia, to LLM bdzie musia dynamicznie na podstawie tego spr贸bowa wykorzysta to do tego, 偶eby wywoa jak akcj i rozszerzy nam funkcjonalnoci. I tutaj od razu taka wa偶na rzecz z naszej praktyki, description warto generowa i opisy do input SCIM-y warto generowa r贸wnie偶 LLM-em po angielsku, 偶eby byy zrozumiae dla LLM-a.

**Szymon Warda**: Warto by leniwym, tak, zdecydowanie, bo te opisy, kt贸re s generowane, s generowane naprawd dobrze i s generowane w taki spos贸b, w jaki LLM to adnie zrozumie. Nie ma co tutaj si wysila, 偶e ja zrobi to lepiej. Nie, nie ma to sensu. Mo偶e jakie mae tweakowanie, ale wtedy to jest bardziej oznaka tego, 偶e co jest niezrozumiae. Jedna jeszcze wa偶na rzecz, bo to jest co, co bdzie si powtarzao przy pozostaych w tych elementach, mianowicie, 偶e wikszo tych resource'u, wikszo tych podstawowych element贸w maj te偶 element odwie偶ania. Czyli, 偶e mo偶e by poinformowany, 偶e ok, jest co nowego, co si zmienio i tak dalej. W toolsach mo偶e ma zastosowanie, ale bdzie miao du偶o wa偶niejsze, wiksze zastosowanie w kontekcie resource'贸w. Ale troch wyprzedzam.

**ukasz Kau偶ny**: Tak i wanie jak wyprzedzasz resourcesy, to kolejny zestaw. Tak, czyli to s, mo偶na powiedzie, 偶e resource'y to s dane bez logiki. Czyli tak jak toolsy, to jest wywoywanie, je偶eli popatrzymy, wywoywanie jakich akcji, modyfikacja, pobieranie, to resource'y to jest po prostu dane bez logiki. Czyli zwr贸 mi jaki API response, jaki kawaek pliku, ewentualnie to s troch dynamiczne 藕r贸da danych, czyli maj jakiego where'a, na przykad zwr贸 mi dane dla u偶ytkownika po ID. Ale to s rzeczy, kt贸re szybko zwracaj dane i s lekkie w obrobieniu.

**Szymon Warda**: Tak, to najczciej koczy si, 偶e to s po prostu pliki, tak w wiekszoci przypadk贸w mo偶na powiedzie. Dobrze, to mielimy takie dwa troch bardziej namacalne. Teraz wchodzimy w promptsy, kt贸re s ju偶 proste jak nie wiem co, bo s jak konsola w Quake'u generalnie z konkretnymi wywoaniami komend tak naprawd.

**ukasz Kau偶ny**: Raczej czy wiesz co, komend, to s predefiniowane snippety, szablony prompt贸w. Czyli we藕my zrobi slash review code i wykona mi albo w zale偶noci jak jest zaimplementowany nasz UI, UX, to model dostanie gotowego prompta do u偶ycia, albo po prostu ten prompt wywietli nam si i rozwinie w naszej konsoli, gdzie wprowadzamy. Czyli gotowe komendy, kt贸re pod spodem s predefiniowanym po prostu szablonem prompta. Nic wicej.

**Szymon Warda**: Tak, 偶eby by pewnym, 偶e model doskonale nas zrozumie. Dobra, to mamy co, co powiedzielimy wczeniej troch, mianowicie sampling, co co weszo relatywnie mo偶na powiedzie nie dawno.

**ukasz Kau偶ny**: Wiesz co, on jest od pocztku, to mo偶e tak, jest od pocztku. Zao偶enie jest takie, 偶e... Cigle jest walka z UI-em i u偶yciem tego prawidowo. To jest chyba dobre okrelenie, bo je偶eli zobaczymy na sampling, jak s klienci, wygldaj gotowi, to z gotowych narzdzi wszystko jest czerwone, nawet Claude ten Chat Desktop, desktopowa apka Cloude'a te偶 tego w og贸le w 偶aden spos贸b nie wspiera i to jest istotne. Ale zao偶enie jest takie, 偶e serwer mo偶e poprosi klienta wanie o taki human in the loop i ca wymian, czyli co bdzie wysane, caa kontrola, co bdzie wysane do LLM-a albo co LLM wykona. Czyli mamy taki cay proces wanie akceptacji, samplowania, co si zadzieje, co bdzie zrobione, co bdzie przekazane, to mo偶na tak bardzo w tym momencie skr贸ci. I tutaj bym chyba powiedzia, 偶e to jest ten moment MCP, gdzie on gdzie jest, ale nie jest okrelany nawet ju偶 jako core feature, jak sobie popatrzymy tam od strony serwer贸w. I cao ma polega na tym, 偶e pozwolimy serwerowi poprosi o co, serwer bdzie m贸g poprosi klienta o co, 偶eby LLM co rozszerzy na przykad i te偶 pozwoli w kt贸rym miejscu na akceptowanie.

**Szymon Warda**: Dobra, to idziemy. Ostatni - roots.

**ukasz Kau偶ny**: Dobra. I to jest taka rzecz od strony klienta, czyli jakie zasoby, jakie cie偶ki dyskowe mog by obsugiwane na przykad lokalnie.

**Szymon Warda**: Proste. Dobra.

**ukasz Kau偶ny**: Albo jeszcze inny przykad, potem zaawansowany, daj dostp do kalendarza czy do czego. To ju偶 takie inne elementy, kt贸re mog si pojawi.

**Szymon Warda**: Dobra, to id藕my kawaek dalej, bo mamy sobie element w MCP je偶eli chodzi o schemat, mamy odnonie transportu. I jeszcze jest kolejny wa偶ny element, mianowicie tego, jak wyglda uwierzytelnianie, bo to s zasoby lokalne, mo偶e nie s takie wa偶ne, ale zdalne to ju偶 musimy wiedzie, jak si ma to wszystko uwierzytelni. Wic jak to miga? Bo miga cakiem nie藕le.

**ukasz Kau偶ny**: Czy wiesz co, bdziemy czy teori z praktyk w tym momencie. Czyli mamy, tak jak powiedziae, stdio lokalnie, krzy偶yk na drog. I teraz zo, kt贸re trzeba powiedzie od strony bezpieczestwa, 偶e bardzo tam czsto tokeny lataj pod spodem. Czyli przy wywoaniu tego lokalnego toola w configu na przykad jawnie deklarujemy clear tekstem. Le偶y sobie na przykad token, uwielbiam taki przykad MCP Servera do Stripe'a, do obsugi patnoci i innych rzeczy, gdzie le偶y token admina? Tak po prostu le偶y sobie goy i wesoy token admina, wic to jest pierwszy przypadek i tutaj 偶ebymy wiedzieli. A kiedy ju偶 p贸jdziemy w te scenariusze bardziej zaawansowane, gdzie mamy t autoryzacj, to tu teraz jest prosto i odpowied藕 brzmi OAuth i OpenID Connect.

**Szymon Warda**: Standard, kt贸ry wykorzystuje kolejny standard mo偶na powiedzie, nic nowego.

**ukasz Kau偶ny**: To nic nowego. Daje nam to gdzie single sign-on'y, znany ju偶 UX, kt贸ry mo偶na wpasowa potem w nasz aplikacj, czyli na przykad zarequest'owa si o zalogowanie do jakiej usugi. We藕my do GitHuba, Office 365, Google Workspace, SAP-a czy innych. Czyli mo偶na kombinowa ze scenariuszami single sign-on, bd藕 poproszenia u偶ytkownika o zalogowanie si. Wic od tej strony nic nie zostao takiego wymylonego. Przy czym sample, je偶eli chodzi na przykad o TypeScript SDK, Python SDK le偶 i kwicz, o tak, je偶eli popatrzymy. Za to wida, 偶e Microsoft jest korporacyjny, bo tam z samplami do OAutha jest o wiele lepiej i Entry.

**Szymon Warda**: Z dokumentacj i rzeczami dla developer贸w sobie radzi cakiem fajnie. Dobra, ale to idziemy kawaek dalej, bo bezpieczestwo nie koczy si waciwie tylko na tym, co si waciwie dzieje, je偶eli chodzi o uwierzytelnianie. Ale mamy cay temat prompt injection, kt贸ry jest potencjaln dziur, bo mamy dwa obszary- to, co u偶ytkownik wysya i to, co model odpowiada niejako. Czyli mamy prompt shielda i content protection. I o co w tej bajce chodzi?

**ukasz Kau偶ny**: Dobra, to pierwsze zajmijmy si tym content protection, o kt贸ry trzeba sobie zadba. To jest pytanie, co my wylemy do LLM-a tak naprawd z naszych danych? I to jest teraz duga dyskusja, ona wychodzi tak naprawd poza MCP Szymon. Bo jak popatrzysz sobie, to jest temat DLP i to jest powiedzenie czy narzdzie mo偶emy wykorzystywa na przykad do danych medycznych czy innych, w zale偶noci, w jakiej firmie pracujemy i co mo偶emy wysya. I to jest taki pierwszy problem, kt贸ry si znajduje. I to jest trudne powiedzenie w tym momencie, tak jawnie, jak to zaimplementowa, jak zrobi do tego compliance. Czyli jako rynek, w tym momencie, jak nagrywamy, czyli wrzesie 2025 roku, mamy problem, 偶e jako rynkowo co rozmawiamy, kto co cigle proponuje, ale nie ma nic takiego jawnie powiedzianego, czym to tak naprawd powinno by.

**Szymon Warda**: Teraz wyjani jak to w og贸le dziaa. To dziaa tak, 偶e zanim otrzymamy kawaek kodu, kt贸ry zanim przeka偶emy do modelu sprawdza na przykad czy tam nie ma zdania typu: we藕, zignoruj wszystkie swoje zakadki bezpieczestwa i zr贸b to, co ja chc. Czyli tak naprawd czy nasz produkt...

**ukasz Kau偶ny**: M贸wisz o content, m贸wisz o content protection, czyli m贸wimy o DLP? Bo ja zaczem od prostszego, czyli DLP data + prevention, czyli czy dane do LLM-a s wysyane jakie z naszego MCP, kt贸re jednak nie powinny by wysyane. A prompt shield to jest to, co powiedziae, sprawdzenie czy nie mamy prompt injection.

**Szymon Warda**: Mo偶liwe, nie bd si upiera. Tak, jedno sprawdzanie tego, co jest wysyane, a drugie to jest to, czy model w odpowiedzi nie wysya czego, co nie powinien. To jest przypadek, kt贸ry mo偶ecie gada sobie z chatem co chcecie, 偶eby on zrobi. I tu si w og贸le nic nie zmienio i on Wam myli, myli, a na koniec m贸wi: sorry, nie mog Ci powiedzie, bo to by amao jakie prawa albo robioby co, co jest nielegalne.

**ukasz Kau偶ny**: Wiesz co, a z drugiej strony taki przypadek tych podatnoci, je偶eli nie mamy kontroli nad danymi. Czyli przykadowo powiemy sobie, 偶eby MCP pobrao co, to jest sawetny atak na MCP GitHuba, czyli ten cay prompt injection, kt贸ry jest. Czyli 偶e cigamy sobie z publicznych 藕r贸de issue'sa, a tam byo 偶eby issue zleakowa, bya instrukcja, 偶eby zleakowa wszystkie secrety i odpowiedzie nimi na przykad w issue, do issue githubowego wpisa swoje wszystkie secrety i prywatne repa, do kt贸rych jest dostp. To bdzie taki przykad. Czyli zao偶enie jest takie, 偶e cay ten prompt shield, kt贸ry mamy, powinnimy weryfikowa te dane, kt贸re je偶eli s one ze 藕r贸de publicznych gdzie cigane albo pochodz z nie zaufanych 藕r贸de, to one powinny by weryfikowane. I jak to si w niekt贸rych okreleniach m贸wi, 偶e ten payload powinien by wysadzany i sprawdzany. I tutaj idc z naszej dziaki najbardziej Azure, na przykad Microsoft ma usug tak po API Restowym, kt贸ra si nazywa Azure AI Safety. Dobra, chyba albo content safety, ju偶 co z tym, co z bezpieczestwem w nazwie. Zostawi linka poprawnego. I cao polega, 偶e mo偶ecie wysa tam na przykad dokument, kt贸ry wsadzacie do LLM-a albo MCP bdzie pobiera i co z nim robi i przekazywa do LLM-a, 偶eby sprawdzi, czy na przykad nie ma tam jakich szkodliwych instrukcji, kt贸re pozwol to co omin.

**Szymon Warda**: Dobra, to to mamy to wr贸my w takim razie do naszego tematu, jak ten MCP w og贸le uo偶y? Czyli jak zrobi, 偶eby on dziaa dobrze? Czyli MCP i API Gateway. Bo o tym troch wspomnielimy, ale troch mo偶e temat rozwimy.

**ukasz Kau偶ny**: Dobra, tu jest du偶y problem i podam to na przykadzie MCP Jiry. MCP od Jiry, tej cloudowej, je偶eli si do niego podepniemy, po prostu zwraca suchajcie as is restowe API. Czyli po prostu wyrzuca kup nieudokumentowanych p贸l, niezrozumiaych opis贸w i innych rzeczy. Po prostu tak jakbycie cignli sobie Postmanem co z Jiry, to wystawia to do LLM-a i LLM g贸wno tego nie rozumie, przepraszam za to okrelenie. Albo ilo, co teraz jest jeszcze gorsza, wikszym wyzwaniem, o czym sobie powiemy, albo ilo token贸w, kt贸re zwraca jest za du偶a i wykorzystujemy limit okna naszego kontekstu input token贸w na to, 偶eby wrzuci syf. I tutaj je偶eli m贸wimy, 偶e jest to API Gateway, to jest to API Gateway dla LLM-贸w i musi by dostosowany, to co bdzie MCP zwracao, musi by dostosowane do LLM-贸w. I to jest taka rzecz kluczowa do zrozumienia. I ja w tym miejscu, suchajcie to co mog Wam poleci, jak rozmawiaem z klientami z naszej praktyki, 偶e to co wystawiamy do LLM-a, to nie jest nasz CRUD z systemu biznesowego czy jak to tam adnie nazwiecie swoje API, kt贸re wystawiacie, tylko przemapowanie tego na faktyczne funkcje biznesowe. Je偶eli co jest niepotrzebne, to nie powinno by wystawiane w tym. Wic o MCP mo偶na pomyle jako API Gateway. Ale to nie jest API Gateway w tym rozumieniu, w kt贸rym go u偶ywalimy.

**Szymon Warda**: Parafrazujc to, co powiedziae, to musi by API, kt贸re bdzie realnie u偶ywalne i kt贸re bdzie dostarczao jak warto, 偶e model mo偶e co tam zrobi, a nie na zasadzie jak CRUD-a, 偶e zr贸b to, zr贸b to, zr贸b to, bo w tym momencie, je偶eli to byby taki prosty CRUD, to nie przekadamy na model ca logik nasz biznesow, kt贸rej on oczywicie nie ma tak naprawd. ... i tyle.

**ukasz Kau偶ny**: Albo co wa偶ne i najgorsze mo偶e si okaza, 偶e model nie trzyma si instrukcji i naszej logiki biznesowej, kt贸r sobie wymylilimy w API, kt贸r na przykad frontend trzyma si doskonale, bo go tam zakodowalimy, jest deterministyczne, ale model nie ma ochoty na to.

**Szymon Warda**: Model bdzie si coraz mniej trzyma tej logiki im du偶szy jest chat, im du偶szy kontekst ma i tak to bdzie wygldao. Nie uciekniemy przed tym. Dobra, czyli powiedzielimy sobie. Mamy jeszcze dwie wa偶ne rzeczy. Mamy jeszcze izolacj i rate limiting.

**ukasz Kau偶ny**: Dobra rate limiting chyba...

**Szymon Warda**: Prosta opcja. Og贸lnie badamy to, 偶eby jeden klient nam nie utuk caego systemu i musimy mie jakie miejsce, gdzie bdziemy m贸wili kto, co, ile zasob贸w zu偶ywa. Co jest super wa偶ne w kontekcie tego jak my pracujemy, to jest to, 偶eby na przykad postawi APIM-a po drodze, 偶eby dokadnie wiedzie kto z czego korzysta, jak to wyglda i 偶eby ten ruch m贸c jako ledzi m贸wic bardzo prosto. Bo potem przyjdzie rachunek i oka偶e si, 偶e sorry, ale nie wiemy kto tyle zu偶y.

**ukasz Kau偶ny**: Dobra. I teraz ten sampling request, ta izolacja od tej strony. Je偶eli popatrzymy. I tutaj zr贸bmy to na przykadzie tego, co jest w dokumentacji od MCP, jak ta izolacja dziaa i sampling. Czyli przykadowo serwer ma list na przykad lot贸w dostpn, przesya je do u偶ytkownika i u偶ytkownik na przykad akceptuje, czy przekaza to potem w og贸le do LLM-a czy nie. To jest jeden taki human in the loop, je偶eli teraz popatrzymy, na tym samplingu. To jest zao偶enie, 偶e klient w kt贸rym miejscu ma t interakcj i tak jawnie powiedzian. Dla mnie, je偶eli kto z Was vibe code'uje czy u偶y Copilota, Cursora, Claude Code, to jest na przykad taka rzecz, kt贸r widzimy, 偶e ja chc wywoa jakie narzdzie albo zedytowa plik. I to jest chyba tak od strony developerskiej do pokazania, to, 偶e mamy gdzie jawne interakcje, gdzie akceptujemy co, a potem jest to koczone tak naprawd w tym. I teraz jest te偶 rozwalenie, bo tam si pojawia wanie nie tylko sampling, ale leak... Wywoania, o tak, po polsku bdzie lepiej, nie bd si mczy z moim sposobem wymowy. Wic pojawiaj si jeszcze, opr贸cz samplingu pojawiaj si wywoania, czyli zao偶enie docelowo w MCP bdzie, 偶e samplign bdzie jednym, wywoania bd drugim. Ale to ju偶 zostawmy w tym momencie drafty w spokoju.

**Szymon Warda**: ukasz m贸wi o elicitation. Dla tych, kt贸rzy chcieliby wygoogla o co chodzi. Dobra, czyli co, to mamy zamknite. To teraz ok, kupilimy czym jest MCP, kupilimy, jak korzysta, do czego jest i tak dalej. To jak teraz w og贸le zacz u偶ywa? Czy to jest na zasadzie zaczynamy od zera czy...

**ukasz Kau偶ny**: Dobra.

**Szymon Warda**: Jak to monitorowa? Jak debugowa?

**ukasz Kau偶ny**: Dobra, mamy dwa podejcia. Po pierwsze YOLO, czyli vibe code'ujemy i to zostawmy. A na powa偶nie to jest pytanie, czy my kodujemy wasne MCP Server? To jest od strony, bo zazwyczaj...

**Szymon Warda**: Wtpliwy sens.

**ukasz Kau偶ny**: Tak. Czy wiesz co, wtpliwy w niekt贸rych momentach bdzie. Je偶eli na przykad robisz produkt i inne rzeczy to nie przeskoczysz.

**Szymon Warda**: Mo偶e tak. Pytanie jak od zera, bo s biblioteki, kt贸re ju偶 daj, s t baz, Serverem MCP i od tego mo偶na zacz.

**ukasz Kau偶ny**: Tak, czyli inaczej, nie kodujemy tego od zera, tylko bierzemy SDK i lecimy z SDK w jzyku, kt贸ry preferujemy w firmie.

**Szymon Warda**: Tak, tam s czasem takie drobne rzeczy typu wanie jak s outy ustawione jakie, jak s standardy speniane i tak dalej. To nie jest tak, 偶e tworzymy nowy serwer HTTP i lecimy od zera. Nie, to nie jest, nie ta droga.

**ukasz Kau偶ny**: Bierzemy SDK i wpinamy si i to bdzie budowao nam. W sumie w wielu przypadkach bdzie to po prostu jaka nakadka i kolejne proxy. I tutaj p贸jd w t stron, idc, odwoujc si do czci azure'owej. Azure API Management, kt贸ry jest takim bardzo du偶ym gatewayem, daje na przykad mo偶liwo wystawienia polityk wanie tych rzeczy, kt贸re ju偶 opublikowalimy, republikacj jako MCP. I to jest taka rzecz, kt贸rej bym si zastanowi teraz, je偶eli na przykad korzystam z Azure'a czy z innych rzeczy, bo te偶 sobie researchowaem, jest par innych, te偶 open source'owych ju偶 takich narzdzi, czy ja nie wystawi po prostu czego istniejcego z drobn zmian logiki na takim proxy API Gateway, 偶eby dostosowa to pod LLM-a. Czyli taki kierunek teraz zawodowy, jak popatrzymy i dowiadczenie m贸wi, 偶e pojawi si teraz produkty i trzy razy bym si zastanowi, czy nie wykorzystam gotowego kawaka usugi albo rozwizania softwareowego, kt贸re pozwoli nam zrobi przemapowanie, ni偶 pisa co od zera.

**Szymon Warda**: Tak, tylko tu jest jedna rzecz wa偶na, to jest to, o czym m贸wilimy, trzeba troch podkreli to, co m贸wilimy wczeniej. To nie jest przemapowanie do jednego. Tam dalej musi by filtracja, dalej musi by zerknicie co, jak, gdzie i co nam to wypluje. Bo wejdzie mas narzdzi, kt贸re powiem, 偶e jak z ka偶d technologi: u偶yj naszego narzdzia i ju偶 w og贸le robota zakoczona. To tak nie do koca dziaa. One uatwiaj prac, ale dalej po naszej stronie musi przej filtracja, jak to w og贸le wyglda i jak to bdzie wykorzystywane.

**ukasz Kau偶ny**: Inaczej, do du偶ej iloci scenariuszy, na przykad od strony Azure'a, mo偶na zaprzc poczenie Azure API Management + Logic Appsy tam, gdzie potrzebujemy troch logiki. Ale jak zaczynamy koczy z workflowem i ifologi i innymi rzeczami, zaczynamy budowa co, co nie przypomina prostej rzeczy: zbierz mi z kilku endpoint贸w na przykad po kolei logik, tylko zaczynamy robi drzewo graf贸w i wyglda to jak klasyczny kod, to lepiej odpali i samodzielnie napisa taki serwer, a potem mo偶na go przeproxowa przez taki API Management. Bo na to API Management pozwala, 偶eby robi te偶 reverse proxy do innych MCP, wic to jest spoko i trzeba znale藕. Je偶eli nie mamy super skomplikowanej logiki, lepiej p贸j w niskokodowe rozwizania. Ale je偶eli zaczynamy robi za du偶o, to ju偶 warto podej do tego. I teraz co trzeba sobie powiedzie o tych funkcjonalnociach API Gateway'贸w? To bdzie istotne. One zakadaj u偶ycie tools贸w, je偶eli popatrzymy. To jest istotny element, 偶e one nastawiaj si na toolsy g贸wnie, troch resource'y. A je偶eli bdziemy chcieli wykorzysta wiksze mo偶liwoci, to prawdopodobnie i tak bdziemy musieli skoczy z samodzielnym odpaleniem serwera MCP.

**Szymon Warda**: Wasnego serwera albo jakie przejci贸wki. Dokadnie tak. Dobra, to co tam jeszcze mamy?

**ukasz Kau偶ny**: Debugger. Debugger, MCP Inspector.

**Szymon Warda**: Przydatny.

**ukasz Kau偶ny**: Szymon, inaczej, dopracowany. To jest w og贸le pikna rzecz.

**Szymon Warda**: I 偶eby to byo jasne, bo w og贸le MCP Inspector to nie jest jaki taki side project, 偶e tak powiem, tylko to faktycznie jest wpisane w dokumentacji jako jedno z narzdzi, kt贸re s preferowane. Wic to jest adnie rozwijane i faktycznie zachowuje si i wyglda dobrze. Tam nie ma popeliny.

**ukasz Kau偶ny**: Jest przyjemny, dziaajcy UI, wspiera uwierzytelnianie. Uwierzytelnianie do serwera, czyli na przykad jak debugujemy lokalnie mo偶emy przej sobie cay flow oauthowy, dostarczy swoj to偶samo i to jest mega dla mnie istotne. Bo na przykad mo偶emy sprawdzi czy wykorzystujemy poprawnie context usera.

**Szymon Warda**: Dobra, to co, lecimy dalej, bo tam jeszcze jest par rzeczy generalnie. S jeszcze narzdzia do przegldania, do debugowania caego JSON-a i takie bardziej niskopoziomowe rzeczy. Ale wikszo sytuacji nasz MCP Inspector wystarczy. Czy co jeszcze?

**ukasz Kau偶ny**: Wiesz co, nie, to bym zostawi, ale poszedbym do troch wyzwa praktycznych, kt贸re bd.

**Szymon Warda**: No to lecimy w takim razie.

**ukasz Kau偶ny**: Czyli z dowiadczenia. I pierwsza rzecz, o kt贸rej powiem, za du偶o narzdzi, za du偶o zwracamy i przez to mamy albo wyczerpanie okna kontekstowego w LLM-ie, albo problemy z trzymaniem si, bo jest za du偶o kontekstu. I to jest chyba rzecz, kt贸r okej, jak tam mamy, to patrz na t sraczk jak nawalimy za du偶o extension'贸w do IDE, 偶eby mie super pikne albo do konsoli, nie wiemy co si dzieje, to tutaj bdzie podobnie.

**Szymon Warda**: Bo to jest znowu taki akt balansowania, bo te偶 mo偶na waln jeden obiekt, jedno wielkie narzdzie z absurdalnym i bardzo skomplikowanym API. To te偶 nie zadziaa do koca.

**ukasz Kau偶ny**: Tak, dlatego m贸wi, 偶e bardziej m贸wi, 偶eby nie byo w tym na przykad tak... Jak tworzymy agenta, to we藕my przypadek korporacyjny, najczstszy, tworzymy agenta, to on tych MCP nie powinien mie nawalone, 偶e zrobi nam wszystko na raz, tylko mo偶e nad nim jest jaki agent orchestrator, kt贸ry woa maego agenta, kt贸ry ma podpite mae MCP, wykorzystane ile tam akcji, a nie jeden wielki agent do zbawienia caego wiata.

**Szymon Warda**: Dobra, to jeszcze jakie mam praktyki dobre, je偶eli chodzi o budowanie samych narzdzi?

**ukasz Kau偶ny**: Dobra, kolejna rzecz, 偶eby pamita o SCIM-ie Validation na inpucie i dobrym opisie SCIM-y inputowej i outputowej. To chyba jest w og贸le najbardziej istotny element caoci.

**Szymon Warda**: Bo model musi wiedzie, co waciwie produkuje, co przyjmuje, czego mo偶e u偶y i co ma zwr贸ci tak naprawd.

**ukasz Kau偶ny**: Tak. Kolejna rzecz to content, 偶eby wspiera obsug r贸偶nych format贸w. I to jest dyskusyjna rzecz, o tak, w jaki spos贸b do tego podchodzimy. Ale z zao偶enia mo偶emy wrzuci tam i tekst i binarki, imige, wic jest ile takich mo偶liwoci tutaj.

**Szymon Warda**: Tak, idziemy dalej. To, o czym m贸wilimy, czyli notyfikacja. M贸wilimy, 偶e te rzeczy wanie jak toolsy, utilities i tak dalej, one mog si zmienia, 偶e to jest szczeg贸lnie wa偶ne przy plikach w kontekcie za贸偶my jak pracujemy, nawet jak ide i tak dalej, 偶eby model by notyfikowany o tym, 偶e co si zmienio. To te偶 jest dobre w kontekcie bycia, 偶eby ten model by bardziej proaktywny, 偶eby tam co si dziao.

**ukasz Kau偶ny**: Raczej to wiesz, te偶 m贸wimy nie model a klient. Czyli 偶e jak idzie nastpny request, to 偶e dziej si nowe rzeczy. I druga sprawa, kt贸ra jest w wyzwaniach praktycznych. Je偶eli mamy long running joby, to serwer powinien odsya progres. Czyli jest tam w tym trzymana sesja, jest odsyany progres. Bo s ciekawe przypadki, ja nauczyem si tego w praktyce, jak mi model zacz halucynowa, jak zaczo si pobieranie, obrabianie jakiego tekstu zwizanego wanie z podcastem. I on na podstawie tytuu, suchaj Szymon, wymyli nam, zsumaryzowa nam odcinek, do kt贸rego nie mia transkryptu.

**Szymon Warda**: Suchaj, ja daem modelowi ostatnio, opcj chatowi, 偶eby zrobi OCR-a. Bardzo adnie, zupenie innymi sowami opisa co jest, co tam na stronie zobaczy. Nie miao to nic wsp贸lnego z tym, co waciwie tam si dziao.

**ukasz Kau偶ny**: I nie mia tej strony.

**Szymon Warda**: Tak, w og贸le, nie wyszo mu to kompletnie. Dobrze, to teraz wracamy na chwil od naszych odskoczni. Odnonie czy dane, kt贸re wysyamy, to pamitajmy te偶 o takich rzeczach typu wanie, 偶e mamy tam jeszcze paginacja albo jeszcze inne rzeczy, wic zwracanie tych danych te偶 mo偶e odbywa si w etapach, nie ma co te偶 wysya. Dobre praktyki developerskie s te偶 mile widziane wszdzie. Dobrze, workspace bundories.

**ukasz Kau偶ny**: Czyli wanie te rooty. Tylko one... Inaczej, znowu p贸jd teraz, rooty s wspania rzecz. Zwr贸c si teraz z powrotem Szymon do client feature'贸w co i jak jest supportowane i pozostawmy to tym minut, zr贸bmy tutaj minut ciszy na temat tego feature'a. Czyli on jest, ale jest to implementowane zazwyczaj ju偶 w samym agencie, narzdziu, a nie w tych gotowcach.

**Szymon Warda**: To wchodzimy na, jeszcze poruszymy temat jeden, czyli autonomiczni agenci, czyli poziom szamaski bardzo wysoki, ale to trzeba powiedzie.

**ukasz Kau偶ny**: Dobra, autonomiczny agent to oznacza, 偶e cron, event albo request go wywoa. To jest autonomiczny agent. On tam nie siedzi i nie myli, wic zostawmy. Problem przy autonomicznych bdzie jeden wielki.

**Szymon Warda**: Monitorowanie.

**ukasz Kau偶ny**: Ale z drugiej strony, monitorowanie tego, wykorzystania tych MCP. I znowu wr贸c do samego pocztku wyzwa, czyli 偶eby mia mao narzdzi, ograniczony scope i nie musia on rozumie pokrtnej logiki biznesowej, je偶eli taka wystpuje, tylko 偶eby mia jak krowie na rowie podane. I druga rzecz, kt贸ra nam wychodzi z tego, to bardzo adnie, nawet jak jest autonomiczny, automagiczny, samodzielny, to na koniec dnia powinien by zaimplementowany czowiek. Przykadowo, 偶e modyfikacja w systemie to by mo偶e jest przygotowanie draftu, wysany mail z akceptacj, 偶e ma zosta co zedytowane, czy propozycje na przykad update'贸w w systemie, kt贸re czowiek zaakceptuje.

**Szymon Warda**: Tak, to co ruszye jest bardzo wa偶ne, bo to nie jest tak, 偶e mo偶emy podczy si, 偶e powinnimy m贸c to, bo nie mo偶emy naszych agent贸w MCP wystawi z jakich system贸w, na przykad korporacyjnych, ksigowych i tak dalej, one bd sobie tam migay, bo automagicznie wyczaj co tam powinno si dzia. To jest co, co powinno by na chwil obecn jeszcze weryfikowane i sprawdzane, czy tam czowiek, odpowiedzialno jest po stronie czowieka.

**ukasz Kau偶ny**: Tak, ja tutaj zostawi bardzo wa偶ne, przypomn, bo to omawialimy, wydaje mi, si przed wakacjami. To bya luka Zero-Click w Copilocie. Ten atak by chyba nazwany EchoLeak, jak dobrze pamitam. I tutaj macie wspaniay przykad, 偶e tw贸rcy technologii sobie z tym nie radz, a co dopiero narze藕biony kawaek agenta modnym frameworkiem LLM-owym.

**Szymon Warda**: Chcc by uczciwym, to ta sekcja odnonie patnoci i w dokumentacji si rozwija i tam jest coraz wicej. I te opisy jak patnoci wygldaj s ju偶 cakiem nieze. Wic podchodz do tego zdecydowanie powa偶nie. To nie jest takie: wyczaicie sobie sami. Dobra, czy mamy co jeszcze waciwie, czy koczymy ten odcinek?

**ukasz Kau偶ny**: Koczymy, bo chyba mo偶na powiedzie, 偶e bierzcie i testujcie. A jak potrzebujecie pomocy w firmie, to zgocie si, bo robimy te偶 to na co dzie.

**Szymon Warda**: Dokadnie tak. Dobra, to co? Tyle, na razie. Heja.

**ukasz Kau偶ny**: Hej!
